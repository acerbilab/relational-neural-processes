{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77392b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "\n",
    "import experiment as exp\n",
    "import lab as B\n",
    "import neuralprocesses.torch as nps\n",
    "import numpy as np\n",
    "import torch\n",
    "import wbml.out as out\n",
    "from matrix.util import ToDenseWarning\n",
    "from wbml.experiment import WorkingDirectory\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "state = B.create_random_state(torch.float32, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "305ed606",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"default\": {\n",
    "            \"epochs\": None,\n",
    "            \"rate\": None,\n",
    "            \"also_ar\": False,\n",
    "        },\n",
    "        \"epsilon\": 1e-8,\n",
    "        \"epsilon_start\": 1e-2,\n",
    "        \"cholesky_retry_factor\": 1e6,\n",
    "        \"fix_noise\": None,\n",
    "        \"fix_noise_epochs\": 3,\n",
    "        \"width\": 256,\n",
    "        \"dim_embedding\": 256,\n",
    "        \"relational_width\": 64,\n",
    "        \"dim_relational_embeddings\": 128,\n",
    "        \"enc_same\": False,\n",
    "        \"num_heads\": 8,\n",
    "        \"num_layers\": 6,\n",
    "        \"num_relational_layers\": 3,\n",
    "        \"unet_channels\": (64,) * 6,\n",
    "        \"unet_strides\": (1,) + (2,) * 5,\n",
    "        \"conv_channels\": 64,\n",
    "        \"encoder_scales\": None,\n",
    "        \"fullconvgnp_kernel_factor\": 2,\n",
    "        \"mean_diff\": 0,\n",
    "        # Performance of the ConvGNP is sensitive to this parameter. Moreover, it\n",
    "        # doesn't make sense to set it to a value higher of the last hidden layer of\n",
    "        # the CNN architecture. We therefore set it to 64.\n",
    "        \"num_basis_functions\": 64,\n",
    "        \"dim_x\": 1\n",
    "    }\n",
    "\n",
    "args = {\"dim_x\": 1,\n",
    "        \"dim_y\": 1,\n",
    "        \"data\": 'eq',\n",
    "        \"batch_size\": 16,\n",
    "        \"epochs\": 100,\n",
    "        \"rate\": 3e-4,\n",
    "        \"objective\": \"loglik\",\n",
    "        \"num_samples\": 20,\n",
    "        \"unnormalised\": False,\n",
    "        \"evaluate_num_samples\": 512,\n",
    "        \"evaluate_batch_size\": 8,\n",
    "        \"train_fast\": False,\n",
    "        \"evaluate_fast\": True,\n",
    "      \n",
    "        \n",
    "       }\n",
    "class mydict(dict):\n",
    "    def __getattribute__(self, key):\n",
    "        if key in self:\n",
    "            return self[key]\n",
    "        else:\n",
    "            return super().__getattribute__(key)    \n",
    "        \n",
    "args = mydict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed42608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train, gen_cv, gens_eval = exp.data[args.data][\"setup\"](\n",
    "        args,\n",
    "        config,\n",
    "        num_tasks_train=2**6, #if args.train_fast else 2**14,\n",
    "        num_tasks_cv=2**6, #if args.train_fast else 2**12,\n",
    "        num_tasks_eval=2**6, #if args.evaluate_fast else 2**12,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41bcaafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "B.epsilon = config['epsilon']\n",
    "\n",
    "# model = nps.construct_gnp(\n",
    "#                 dim_x=config[\"dim_x\"],\n",
    "#                 dim_yc=(1,) * config[\"dim_y\"],\n",
    "#                 dim_yt=config[\"dim_y\"],\n",
    "#                 dim_embedding=config[\"dim_embedding\"],\n",
    "#                 enc_same=config[\"enc_same\"],\n",
    "#                 num_dec_layers=config[\"num_layers\"],\n",
    "#                 width=config[\"width\"],\n",
    "#                 likelihood=\"het\",\n",
    "#                 transform=config[\"transform\"],\n",
    "#             )\n",
    "\n",
    "model = nps.construct_rnp(\n",
    "                dim_x=config[\"dim_x\"],\n",
    "                dim_yc=(1,) * config[\"dim_y\"],\n",
    "                dim_yt=config[\"dim_y\"],\n",
    "                dim_embedding=config[\"dim_embedding\"],\n",
    "                enc_same=config[\"enc_same\"],\n",
    "                num_dec_layers=config[\"num_layers\"],\n",
    "                width=config[\"width\"],\n",
    "                relational_width=config['relational_width'],\n",
    "                num_relational_enc_layers=config['num_relational_layers'],\n",
    "                likelihood=\"het\",\n",
    "                transform=config[\"transform\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8349b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = partial(\n",
    "            nps.loglik,\n",
    "            num_samples=args.num_samples,\n",
    "            normalise=not args.unnormalised,\n",
    "        )\n",
    "objective_cv = partial(\n",
    "            nps.loglik,\n",
    "            num_samples=args.num_samples,\n",
    "            normalise=not args.unnormalised,\n",
    "        )\n",
    "objectives_eval = [\n",
    "            (\n",
    "                \"Loglik\",\n",
    "                partial(\n",
    "                    nps.loglik,\n",
    "                    num_samples=args.evaluate_num_samples,\n",
    "                    batch_size=args.evaluate_batch_size,\n",
    "                    normalise=not args.unnormalised,\n",
    "                ),\n",
    "            )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54908b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(state, model, opt, objective, gen, *, fix_noise):\n",
    "    \"\"\"Train for an epoch.\"\"\"\n",
    "    vals = []\n",
    "    for batch in gen.epoch():\n",
    "        state, obj = objective(\n",
    "            state,\n",
    "            model,\n",
    "            batch[\"contexts\"],\n",
    "            batch[\"xt\"],\n",
    "            batch[\"yt\"],\n",
    "            fix_noise=fix_noise,\n",
    "        )\n",
    "        vals.append(B.to_numpy(obj))\n",
    "        # Be sure to negate the output of `objective`.\n",
    "        val = -B.mean(obj)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        val.backward()\n",
    "        opt.step()\n",
    "\n",
    "    vals = B.concat(*vals)\n",
    "    out.kv(\"Loglik (T)\", exp.with_err(vals, and_lower=True))\n",
    "    return state, B.mean(vals) - 1.96 * B.std(vals) / B.sqrt(len(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a23aad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(state, model, objective, gen):\n",
    "    \"\"\"Perform evaluation.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        vals, kls, kls_diag = [], [], []\n",
    "        for batch in gen.epoch():\n",
    "            state, obj = objective(\n",
    "                state,\n",
    "                model,\n",
    "                batch[\"contexts\"],\n",
    "                batch[\"xt\"],\n",
    "                batch[\"yt\"],\n",
    "            )\n",
    "\n",
    "            # Save numbers.\n",
    "            n = nps.num_data(batch[\"xt\"], batch[\"yt\"])\n",
    "            vals.append(B.to_numpy(obj))\n",
    "            if \"pred_logpdf\" in batch:\n",
    "                kls.append(B.to_numpy(batch[\"pred_logpdf\"] / n - obj))\n",
    "            if \"pred_logpdf_diag\" in batch:\n",
    "                kls_diag.append(B.to_numpy(batch[\"pred_logpdf_diag\"] / n - obj))\n",
    "\n",
    "        # Report numbers.\n",
    "        vals = B.concat(*vals)\n",
    "        out.kv(\"Loglik (V)\", exp.with_err(vals, and_lower=True))\n",
    "        if kls:\n",
    "            out.kv(\"KL (full)\", exp.with_err(B.concat(*kls), and_upper=True))\n",
    "        if kls_diag:\n",
    "            out.kv(\"KL (diag)\", exp.with_err(B.concat(*kls_diag), and_upper=True))\n",
    "\n",
    "        return state, B.mean(vals) - 1.96 * B.std(vals) / B.sqrt(len(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2638912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "    Loglik (T):   -1.36545 +-    0.05478 (  -1.42023)\n",
      "    Loglik (V):   -1.38369 +-    0.04967 (  -1.43336)\n",
      "    KL (full):     1.05762 +-    0.04882 (   1.10644)\n",
      "    KL (diag):     0.63488 +-    0.06960 (   0.70448)\n",
      "Epoch 2:\n",
      "    Loglik (T):   -1.43593 +-    0.06178 (  -1.49771)\n",
      "    Loglik (V):   -1.38485 +-    0.04948 (  -1.43433)\n",
      "    KL (full):     1.05878 +-    0.04881 (   1.10759)\n",
      "    KL (diag):     0.63604 +-    0.06946 (   0.70550)\n",
      "Epoch 3:\n",
      "    Loglik (T):   -1.38601 +-    0.05773 (  -1.44375)\n",
      "    Loglik (V):   -1.39722 +-    0.05429 (  -1.45152)\n",
      "    KL (full):     1.07115 +-    0.05304 (   1.12419)\n",
      "    KL (diag):     0.64841 +-    0.07175 (   0.72017)\n",
      "Epoch 4:\n",
      "    Loglik (T):   -1.38060 +-    0.05902 (  -1.43962)\n",
      "    Loglik (V):   -1.39947 +-    0.05385 (  -1.45333)\n",
      "    KL (full):     1.07340 +-    0.05272 (   1.12612)\n",
      "    KL (diag):     0.65066 +-    0.07150 (   0.72217)\n",
      "Epoch 5:\n",
      "    Loglik (T):   -1.37597 +-    0.05067 (  -1.42663)\n",
      "    Loglik (V):   -1.39989 +-    0.05098 (  -1.45086)\n",
      "    KL (full):     1.07381 +-    0.05035 (   1.12416)\n",
      "    KL (diag):     0.65108 +-    0.07034 (   0.72142)\n",
      "Epoch 6:\n",
      "    Loglik (T):   -1.37824 +-    0.06006 (  -1.43831)\n",
      "    Loglik (V):   -1.38484 +-    0.04917 (  -1.43401)\n",
      "    KL (full):     1.05877 +-    0.04854 (   1.10731)\n",
      "    KL (diag):     0.63603 +-    0.06898 (   0.70501)\n",
      "Epoch 7:\n",
      "    Loglik (T):   -1.39134 +-    0.06303 (  -1.45437)\n",
      "    Loglik (V):   -1.38441 +-    0.05052 (  -1.43492)\n",
      "    KL (full):     1.05833 +-    0.04991 (   1.10825)\n",
      "    KL (diag):     0.63560 +-    0.06990 (   0.70549)\n",
      "Epoch 8:\n",
      "    Loglik (T):   -1.44191 +-    0.05908 (  -1.50098)\n",
      "    Loglik (V):   -1.38303 +-    0.05028 (  -1.43331)\n",
      "    KL (full):     1.05696 +-    0.04994 (   1.10690)\n",
      "    KL (diag):     0.63422 +-    0.07039 (   0.70461)\n",
      "Epoch 9:\n",
      "    Loglik (T):   -1.41714 +-    0.05099 (  -1.46813)\n",
      "    Loglik (V):   -1.38433 +-    0.05134 (  -1.43566)\n",
      "    KL (full):     1.05825 +-    0.05023 (   1.10849)\n",
      "    KL (diag):     0.63552 +-    0.07003 (   0.70555)\n",
      "Epoch 10:\n",
      "    Loglik (T):   -1.37976 +-    0.05649 (  -1.43625)\n",
      "    Loglik (V):   -1.38533 +-    0.05156 (  -1.43688)\n",
      "    KL (full):     1.05926 +-    0.05039 (   1.10965)\n",
      "    KL (diag):     0.63652 +-    0.06951 (   0.70603)\n",
      "Epoch 11:\n",
      "    Loglik (T):   -1.40033 +-    0.05537 (  -1.45571)\n",
      "    Loglik (V):   -1.38944 +-    0.05080 (  -1.44024)\n",
      "    KL (full):     1.06337 +-    0.04993 (   1.11330)\n",
      "    KL (diag):     0.64063 +-    0.06925 (   0.70988)\n",
      "Epoch 12:\n",
      "    Loglik (T):   -1.38442 +-    0.05461 (  -1.43902)\n",
      "    Loglik (V):   -1.39011 +-    0.04957 (  -1.43968)\n",
      "    KL (full):     1.06404 +-    0.04903 (   1.11307)\n",
      "    KL (diag):     0.64130 +-    0.06898 (   0.71028)\n",
      "Epoch 13:\n",
      "    Loglik (T):   -1.40417 +-    0.05187 (  -1.45604)\n",
      "    Loglik (V):   -1.38281 +-    0.04948 (  -1.43229)\n",
      "    KL (full):     1.05673 +-    0.04869 (   1.10542)\n",
      "    KL (diag):     0.63400 +-    0.06857 (   0.70256)\n",
      "Epoch 14:\n",
      "    Loglik (T):   -1.41734 +-    0.05982 (  -1.47716)\n",
      "    Loglik (V):   -1.37853 +-    0.04701 (  -1.42555)\n",
      "    KL (full):     1.05246 +-    0.04645 (   1.09891)\n",
      "    KL (diag):     0.62972 +-    0.06777 (   0.69749)\n",
      "Epoch 15:\n",
      "    Loglik (T):   -1.39965 +-    0.06464 (  -1.46429)\n",
      "    Loglik (V):   -1.38245 +-    0.05004 (  -1.43250)\n",
      "    KL (full):     1.05638 +-    0.04957 (   1.10595)\n",
      "    KL (diag):     0.63364 +-    0.06940 (   0.70304)\n",
      "Epoch 16:\n",
      "    Loglik (T):   -1.42323 +-    0.06265 (  -1.48588)\n",
      "    Loglik (V):   -1.38041 +-    0.04984 (  -1.43025)\n",
      "    KL (full):     1.05433 +-    0.04934 (   1.10367)\n",
      "    KL (diag):     0.63160 +-    0.06922 (   0.70082)\n",
      "Epoch 17:\n",
      "    Loglik (T):   -1.35384 +-    0.04199 (  -1.39583)\n",
      "    Loglik (V):   -1.37843 +-    0.05061 (  -1.42904)\n",
      "    KL (full):     1.05236 +-    0.04994 (   1.10229)\n",
      "    KL (diag):     0.62962 +-    0.06935 (   0.69897)\n",
      "Epoch 18:\n",
      "    Loglik (T):   -1.39687 +-    0.05940 (  -1.45627)\n",
      "    Loglik (V):   -1.37406 +-    0.04947 (  -1.42353)\n",
      "    KL (full):     1.04799 +-    0.04882 (   1.09681)\n",
      "    KL (diag):     0.62525 +-    0.06906 (   0.69431)\n",
      "Epoch 19:\n",
      "    Loglik (T):   -1.42080 +-    0.05922 (  -1.48001)\n",
      "    Loglik (V):   -1.37764 +-    0.04995 (  -1.42759)\n",
      "    KL (full):     1.05157 +-    0.04921 (   1.10078)\n",
      "    KL (diag):     0.62883 +-    0.06958 (   0.69841)\n",
      "Epoch 20:\n",
      "    Loglik (T):   -1.36975 +-    0.05278 (  -1.42254)\n",
      "    Loglik (V):   -1.39103 +-    0.05539 (  -1.44642)\n",
      "    KL (full):     1.06496 +-    0.05420 (   1.11915)\n",
      "    KL (diag):     0.64222 +-    0.07209 (   0.71431)\n",
      "Epoch 21:\n",
      "    Loglik (T):   -1.36401 +-    0.05877 (  -1.42277)\n",
      "    Loglik (V):   -1.38778 +-    0.05340 (  -1.44118)\n",
      "    KL (full):     1.06171 +-    0.05240 (   1.11411)\n",
      "    KL (diag):     0.63897 +-    0.07092 (   0.70989)\n",
      "Epoch 22:\n",
      "    Loglik (T):   -1.35979 +-    0.05069 (  -1.41048)\n",
      "    Loglik (V):   -1.38186 +-    0.04857 (  -1.43043)\n",
      "    KL (full):     1.05578 +-    0.04801 (   1.10379)\n",
      "    KL (diag):     0.63305 +-    0.06872 (   0.70177)\n",
      "Epoch 23:\n",
      "    Loglik (T):   -1.38431 +-    0.06724 (  -1.45155)\n",
      "    Loglik (V):   -1.37965 +-    0.04834 (  -1.42799)\n",
      "    KL (full):     1.05358 +-    0.04774 (   1.10132)\n",
      "    KL (diag):     0.63084 +-    0.06871 (   0.69954)\n",
      "Epoch 24:\n",
      "    Loglik (T):   -1.34110 +-    0.05873 (  -1.39983)\n",
      "    Loglik (V):   -1.38453 +-    0.05390 (  -1.43843)\n",
      "    KL (full):     1.05845 +-    0.05319 (   1.11165)\n",
      "    KL (diag):     0.63572 +-    0.07135 (   0.70706)\n",
      "Epoch 25:\n",
      "    Loglik (T):   -1.36829 +-    0.05837 (  -1.42666)\n",
      "    Loglik (V):   -1.38314 +-    0.05425 (  -1.43739)\n",
      "    KL (full):     1.05707 +-    0.05318 (   1.11025)\n",
      "    KL (diag):     0.63433 +-    0.07107 (   0.70541)\n",
      "Epoch 26:\n",
      "    Loglik (T):   -1.37127 +-    0.06178 (  -1.43305)\n",
      "    Loglik (V):   -1.37846 +-    0.05041 (  -1.42888)\n",
      "    KL (full):     1.05239 +-    0.04921 (   1.10160)\n",
      "    KL (diag):     0.62965 +-    0.06929 (   0.69894)\n",
      "Epoch 27:\n",
      "    Loglik (T):   -1.40983 +-    0.05673 (  -1.46656)\n",
      "    Loglik (V):   -1.37458 +-    0.04978 (  -1.42436)\n",
      "    KL (full):     1.04850 +-    0.04877 (   1.09727)\n",
      "    KL (diag):     0.62577 +-    0.06903 (   0.69480)\n",
      "Epoch 28:\n",
      "    Loglik (T):   -1.33915 +-    0.05122 (  -1.39037)\n",
      "    Loglik (V):   -1.38161 +-    0.05522 (  -1.43683)\n",
      "    KL (full):     1.05554 +-    0.05410 (   1.10964)\n",
      "    KL (diag):     0.63280 +-    0.07178 (   0.70458)\n",
      "Epoch 29:\n",
      "    Loglik (T):   -1.44681 +-    0.06269 (  -1.50950)\n",
      "    Loglik (V):   -1.38027 +-    0.05270 (  -1.43297)\n",
      "    KL (full):     1.05419 +-    0.05174 (   1.10593)\n",
      "    KL (diag):     0.63145 +-    0.07075 (   0.70220)\n",
      "Epoch 30:\n",
      "    Loglik (T):   -1.40462 +-    0.05195 (  -1.45657)\n",
      "    Loglik (V):   -1.38072 +-    0.04606 (  -1.42679)\n",
      "    KL (full):     1.05465 +-    0.04595 (   1.10060)\n",
      "    KL (diag):     0.63191 +-    0.06829 (   0.70020)\n",
      "Epoch 31:\n",
      "    Loglik (T):   -1.37004 +-    0.05032 (  -1.42036)\n",
      "    Loglik (V):   -1.38701 +-    0.05049 (  -1.43750)\n",
      "    KL (full):     1.06093 +-    0.05025 (   1.11119)\n",
      "    KL (diag):     0.63820 +-    0.07048 (   0.70868)\n",
      "Epoch 32:\n",
      "    Loglik (T):   -1.38889 +-    0.06426 (  -1.45315)\n",
      "    Loglik (V):   -1.38029 +-    0.05324 (  -1.43352)\n",
      "    KL (full):     1.05421 +-    0.05187 (   1.10608)\n",
      "    KL (diag):     0.63148 +-    0.07086 (   0.70233)\n",
      "Epoch 33:\n",
      "    Loglik (T):   -1.40060 +-    0.05902 (  -1.45961)\n",
      "    Loglik (V):   -1.38082 +-    0.05249 (  -1.43331)\n",
      "    KL (full):     1.05475 +-    0.05104 (   1.10579)\n",
      "    KL (diag):     0.63201 +-    0.07057 (   0.70259)\n",
      "Epoch 34:\n",
      "    Loglik (T):   -1.38629 +-    0.05900 (  -1.44529)\n",
      "    Loglik (V):   -1.37601 +-    0.04775 (  -1.42376)\n",
      "    KL (full):     1.04994 +-    0.04729 (   1.09722)\n",
      "    KL (diag):     0.62720 +-    0.06826 (   0.69546)\n",
      "Epoch 35:\n",
      "    Loglik (T):   -1.43687 +-    0.06394 (  -1.50081)\n",
      "    Loglik (V):   -1.38163 +-    0.04454 (  -1.42617)\n",
      "    KL (full):     1.05556 +-    0.04495 (   1.10051)\n",
      "    KL (diag):     0.63282 +-    0.06756 (   0.70039)\n",
      "Epoch 36:\n",
      "    Loglik (T):   -1.42751 +-    0.05090 (  -1.47841)\n",
      "    Loglik (V):   -1.37791 +-    0.04612 (  -1.42403)\n",
      "    KL (full):     1.05184 +-    0.04593 (   1.09777)\n",
      "    KL (diag):     0.62910 +-    0.06785 (   0.69695)\n",
      "Epoch 37:\n",
      "    Loglik (T):   -1.40136 +-    0.05369 (  -1.45505)\n",
      "    Loglik (V):   -1.37833 +-    0.05081 (  -1.42914)\n",
      "    KL (full):     1.05226 +-    0.04990 (   1.10216)\n",
      "    KL (diag):     0.62952 +-    0.06951 (   0.69903)\n",
      "Epoch 38:\n",
      "    Loglik (T):   -1.42312 +-    0.05128 (  -1.47440)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loglik (V):   -1.38331 +-    0.05267 (  -1.43598)\n",
      "    KL (full):     1.05724 +-    0.05211 (   1.10935)\n",
      "    KL (diag):     0.63450 +-    0.07062 (   0.70512)\n",
      "Epoch 39:\n",
      "    Loglik (T):   -1.34827 +-    0.05595 (  -1.40422)\n",
      "    Loglik (V):   -1.37987 +-    0.04791 (  -1.42779)\n",
      "    KL (full):     1.05380 +-    0.04793 (   1.10173)\n",
      "    KL (diag):     0.63106 +-    0.06889 (   0.69995)\n",
      "Epoch 40:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     fix_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m state, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgen_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# The epoch is done. Now evaluate.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m state, val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(state, model, objective_cv, gen_cv())\n",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(state, model, opt, objective, gen, fix_noise)\u001b[0m\n\u001b[1;32m     16\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m     val\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m vals \u001b[38;5;241m=\u001b[39m B\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;241m*\u001b[39mvals)\n\u001b[1;32m     21\u001b[0m out\u001b[38;5;241m.\u001b[39mkv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoglik (T)\u001b[39m\u001b[38;5;124m\"\u001b[39m, exp\u001b[38;5;241m.\u001b[39mwith_err(vals, and_lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/anaconda3/envs/np/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/np/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/np/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/np/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/np/lib/python3.10/site-packages/torch/optim/adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    389\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    393\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "\n",
    "best_eval_lik = -np.inf\n",
    "\n",
    "# Setup training loop.\n",
    "opt = torch.optim.Adam(model.parameters(), args.rate)\n",
    "\n",
    "# Set regularisation high for the first epochs.\n",
    "original_epsilon = B.epsilon\n",
    "B.epsilon = config[\"epsilon_start\"]\n",
    "\n",
    "for i in range(start, args.epochs):\n",
    "    with out.Section(f\"Epoch {i + 1}\"):\n",
    "        # Set regularisation to normal after the first epoch.\n",
    "        if i > 0:\n",
    "            B.epsilon = original_epsilon\n",
    "\n",
    "        # Perform an epoch.\n",
    "        if config[\"fix_noise\"] and i < config[\"fix_noise_epochs\"]:\n",
    "            fix_noise = 1e-4\n",
    "        else:\n",
    "            fix_noise = None\n",
    "        state, _ = train(\n",
    "            state,\n",
    "            model,\n",
    "            opt,\n",
    "            objective,\n",
    "            gen_train,\n",
    "            fix_noise=fix_noise,\n",
    "        )\n",
    "\n",
    "        # The epoch is done. Now evaluate.\n",
    "        state, val = eval(state, model, objective_cv, gen_cv())\n",
    "\n",
    "        # Save current model.\n",
    "#         torch.save(\n",
    "#             {\n",
    "#                 \"weights\": model.state_dict(),\n",
    "#                 \"objective\": val,\n",
    "#                 \"epoch\": i + 1,\n",
    "#             },\n",
    "#             wd.file(f\"model-last.torch\"),\n",
    "#         )\n",
    "\n",
    "        # Check if the model is the new best. If so, save it.\n",
    "#         if val > best_eval_lik:\n",
    "#             out.out(\"New best model!\")\n",
    "#             best_eval_lik = val\n",
    "#             torch.save(\n",
    "#                 {\n",
    "#                     \"weights\": model.state_dict(),\n",
    "#                     \"objective\": val,\n",
    "#                     \"epoch\": i + 1,\n",
    "#                 },\n",
    "#                 wd.file(f\"model-best.torch\"),\n",
    "#             )\n",
    "\n",
    "        # Visualise a few predictions by the model.\n",
    "#         gen = gen_cv()\n",
    "#         for j in range(5):\n",
    "#             path = (f\"result/train-epoch-{i + 1:03d}-{j + 1}.pdf\") \n",
    "#             exp.visualise(\n",
    "#                 model,\n",
    "#                 gen,\n",
    "#                 path=path,\n",
    "#                 config=config,\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec47d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-np]",
   "language": "python",
   "name": "conda-env-.conda-np-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
