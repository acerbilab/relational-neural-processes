{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to compare the performance between original RCNP and our implementation is to replace the train function and its corresponding loglik objective with our own methods, while remaining other functions the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D regression task\n",
    "\n",
    "We start by importing the necessary dependencies. This implementation is based on PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "import experiment as exp\n",
    "import lab as B\n",
    "import wbml.out as out\n",
    "from matrix.util import ToDenseWarning\n",
    "from wbml.experiment import WorkingDirectory\n",
    "import neuralprocesses.torch as nps\n",
    "from neuralprocesses.numdata import num_data\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "device = torch.device(\"cuda\") if USE_CUDA else torch.device(\"cpu\")\n",
    "\n",
    "state = B.create_random_state(torch.float32, seed=0)\n",
    "B.set_global_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"default\": {\n",
    "            \"epochs\": None,\n",
    "            \"rate\": None,\n",
    "            \"also_ar\": False,\n",
    "        },\n",
    "        \"epsilon\": 1e-8,\n",
    "        \"epsilon_start\": 1e-2,\n",
    "        \"cholesky_retry_factor\": 1e6,\n",
    "        \"fix_noise\": None,\n",
    "        \"fix_noise_epochs\": 3,\n",
    "        \"width\": 256,\n",
    "        \"dim_embedding\": 256,\n",
    "        \"relational_width\": 64,\n",
    "        \"dim_relational_embeddings\": 128,\n",
    "        \"enc_same\": False,\n",
    "        \"num_heads\": 8,\n",
    "        \"num_layers\": 6,\n",
    "        \"num_relational_layers\": 3,\n",
    "        \"unet_channels\": (64,) * 6,\n",
    "        \"unet_strides\": (1,) + (2,) * 5,\n",
    "        \"conv_channels\": 64,\n",
    "        \"encoder_scales\": None,\n",
    "        \"fullconvgnp_kernel_factor\": 2,\n",
    "        \"mean_diff\": 0,\n",
    "        # Performance of the ConvGNP is sensitive to this parameter. Moreover, it\n",
    "        # doesn't make sense to set it to a value higher of the last hidden layer of\n",
    "        # the CNN architecture. We therefore set it to 64.\n",
    "        \"num_basis_functions\": 64,\n",
    "        \"dim_x\": 1\n",
    "    }\n",
    "\n",
    "args = {\"dim_x\": 1,\n",
    "        \"dim_y\": 1,\n",
    "        \"data\": 'eq',\n",
    "        \"batch_size\": 16,\n",
    "        \"epochs\": 100,\n",
    "        \"rate\": 3e-4,\n",
    "        \"objective\": \"loglik\",\n",
    "        \"num_samples\": 20,\n",
    "        \"unnormalised\": False,\n",
    "        \"evaluate_num_samples\": 512,\n",
    "        \"evaluate_batch_size\": 8,\n",
    "        \"train_fast\": False,\n",
    "        \"evaluate_fast\": True,\n",
    "      \n",
    "        \n",
    "       }\n",
    "class mydict(dict):\n",
    "    def __getattribute__(self, key):\n",
    "        if key in self:\n",
    "            return self[key]\n",
    "        else:\n",
    "            return super().__getattribute__(key)    \n",
    "        \n",
    "args = mydict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define below some global variables for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train, gen_cv, gens_eval = exp.data[args.data][\"setup\"](\n",
    "        args,\n",
    "        config,\n",
    "        num_tasks_train=2**6 if args.train_fast else 2**14,\n",
    "        num_tasks_cv=2**6 if args.train_fast else 2**12,\n",
    "        num_tasks_eval=2**6 if args.evaluate_fast else 2**12,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NP package train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = partial(\n",
    "            nps.loglik,\n",
    "            num_samples=args.num_samples,\n",
    "            normalise=not args.unnormalised,\n",
    "        )\n",
    "objective_cv = partial(\n",
    "            nps.loglik,\n",
    "            num_samples=args.num_samples,\n",
    "            normalise=not args.unnormalised,\n",
    "        )\n",
    "objectives_eval = [\n",
    "            (\n",
    "                \"Loglik\",\n",
    "                partial(\n",
    "                    nps.loglik,\n",
    "                    num_samples=args.evaluate_num_samples,\n",
    "                    batch_size=args.evaluate_batch_size,\n",
    "                    normalise=not args.unnormalised,\n",
    "                ),\n",
    "            )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(state, model, opt, objective, gen, *, fix_noise):\n",
    "    \"\"\"Train for an epoch.\"\"\"\n",
    "    vals = []\n",
    "    for batch in gen.epoch():\n",
    "        state, obj = objective(\n",
    "            state,\n",
    "            model,\n",
    "            batch[\"contexts\"],\n",
    "            batch[\"xt\"],\n",
    "            batch[\"yt\"],\n",
    "            fix_noise=fix_noise,\n",
    "        )\n",
    "        vals.append(B.to_numpy(obj))\n",
    "        # Be sure to negate the output of `objective`.\n",
    "        val = -B.mean(obj)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        val.backward()\n",
    "        opt.step()\n",
    "\n",
    "    vals = B.concat(*vals)\n",
    "    out.kv(\"Loglik (T)\", exp.with_err(vals, and_lower=True))\n",
    "    return state, B.mean(vals) - 1.96 * B.std(vals) / B.sqrt(len(vals))\n",
    "\n",
    "def eval(state, model, objective, gen):\n",
    "    \"\"\"Perform evaluation.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        vals, kls, kls_diag = [], [], []\n",
    "        for batch in gen.epoch():\n",
    "            state, obj = objective(\n",
    "                state,\n",
    "                model,\n",
    "                batch[\"contexts\"],\n",
    "                batch[\"xt\"],\n",
    "                batch[\"yt\"],\n",
    "            )\n",
    "\n",
    "            # Save numbers.\n",
    "            n = nps.num_data(batch[\"xt\"], batch[\"yt\"])\n",
    "            vals.append(B.to_numpy(obj))\n",
    "            if \"pred_logpdf\" in batch:\n",
    "                kls.append(B.to_numpy(batch[\"pred_logpdf\"] / n - obj))\n",
    "            if \"pred_logpdf_diag\" in batch:\n",
    "                kls_diag.append(B.to_numpy(batch[\"pred_logpdf_diag\"] / n - obj))\n",
    "\n",
    "        # Report numbers.\n",
    "        vals = B.concat(*vals)\n",
    "        out.kv(\"Loglik (V)\", exp.with_err(vals, and_lower=True))\n",
    "        if kls:\n",
    "            out.kv(\"KL (full)\", exp.with_err(B.concat(*kls), and_upper=True))\n",
    "        if kls_diag:\n",
    "            out.kv(\"KL (diag)\", exp.with_err(B.concat(*kls_diag), and_upper=True))\n",
    "        \n",
    "        # objective doesn't return pred_y, we can't plot the data\n",
    "\n",
    "        return state, B.mean(vals) - 1.96 * B.std(vals) / B.sqrt(len(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.epsilon = config['epsilon']\n",
    "\n",
    "model = nps.construct_rnp(\n",
    "                dim_x=config[\"dim_x\"],\n",
    "                dim_yc=(1,) * config[\"dim_y\"],\n",
    "                dim_yt=config[\"dim_y\"],\n",
    "                dim_embedding=config[\"dim_embedding\"],\n",
    "                enc_same=config[\"enc_same\"],\n",
    "                num_dec_layers=config[\"num_layers\"],\n",
    "                width=config[\"width\"],\n",
    "                relational_width=config['relational_width'],\n",
    "                num_relational_enc_layers=config['num_relational_layers'],\n",
    "                likelihood=\"het\",\n",
    "                transform=config[\"transform\"],\n",
    "            )\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "    Loglik (T):   -1.22006 +-    0.00481 (  -1.22488)\n",
      "    Loglik (V):   -0.99503 +-    0.00976 (  -1.00479)\n",
      "    KL (full):     0.68121 +-    0.00833 (   0.68954)\n",
      "    KL (diag):     0.32053 +-    0.00776 (   0.32829)\n",
      "Epoch 2:\n",
      "    Loglik (T):   -0.90953 +-    0.00489 (  -0.91442)\n",
      "    Loglik (V):   -0.82819 +-    0.01027 (  -0.83846)\n",
      "    KL (full):     0.59246 +-    0.00811 (   0.60057)\n",
      "    KL (diag):     0.20899 +-    0.00603 (   0.21502)\n",
      "Epoch 3:\n",
      "    Loglik (T):   -0.80329 +-    0.00502 (  -0.80831)\n",
      "    Loglik (V):   -0.75287 +-    0.01044 (  -0.76331)\n",
      "    KL (full):     0.51714 +-    0.00784 (   0.52498)\n",
      "    KL (diag):     0.13367 +-    0.00437 (   0.13804)\n",
      "Epoch 4:\n",
      "    Loglik (T):   -0.78931 +-    0.00525 (  -0.79456)\n",
      "    Loglik (V):   -0.74758 +-    0.01012 (  -0.75771)\n",
      "    KL (full):     0.51185 +-    0.00751 (   0.51936)\n",
      "    KL (diag):     0.12839 +-    0.00412 (   0.13251)\n",
      "Epoch 5:\n",
      "    Loglik (T):   -0.75829 +-    0.00516 (  -0.76346)\n",
      "    Loglik (V):   -0.75133 +-    0.01002 (  -0.76134)\n",
      "    KL (full):     0.51560 +-    0.00738 (   0.52298)\n",
      "    KL (diag):     0.13213 +-    0.00410 (   0.13623)\n",
      "Epoch 6:\n",
      "    Loglik (T):   -0.76714 +-    0.00519 (  -0.77232)\n",
      "    Loglik (V):   -0.73529 +-    0.01020 (  -0.74549)\n",
      "    KL (full):     0.49956 +-    0.00754 (   0.50709)\n",
      "    KL (diag):     0.11609 +-    0.00392 (   0.12001)\n",
      "Epoch 7:\n",
      "    Loglik (T):   -0.77256 +-    0.00533 (  -0.77789)\n",
      "    Loglik (V):   -0.73315 +-    0.01017 (  -0.74332)\n",
      "    KL (full):     0.49742 +-    0.00753 (   0.50495)\n",
      "    KL (diag):     0.11395 +-    0.00391 (   0.11786)\n",
      "Epoch 8:\n",
      "    Loglik (T):   -0.77450 +-    0.00530 (  -0.77980)\n",
      "    Loglik (V):   -0.73288 +-    0.01043 (  -0.74330)\n",
      "    KL (full):     0.49715 +-    0.00776 (   0.50491)\n",
      "    KL (diag):     0.11368 +-    0.00408 (   0.11776)\n",
      "Epoch 9:\n",
      "    Loglik (T):   -0.74115 +-    0.00516 (  -0.74631)\n",
      "    Loglik (V):   -0.73797 +-    0.01028 (  -0.74825)\n",
      "    KL (full):     0.50224 +-    0.00761 (   0.50985)\n",
      "    KL (diag):     0.11877 +-    0.00392 (   0.12270)\n",
      "Epoch 10:\n",
      "    Loglik (T):   -0.74395 +-    0.00519 (  -0.74914)\n",
      "    Loglik (V):   -0.73358 +-    0.01023 (  -0.74381)\n",
      "    KL (full):     0.49785 +-    0.00754 (   0.50539)\n",
      "    KL (diag):     0.11438 +-    0.00385 (   0.11824)\n",
      "Epoch 11:\n",
      "    Loglik (T):   -0.74759 +-    0.00520 (  -0.75278)\n",
      "    Loglik (V):   -0.72280 +-    0.01029 (  -0.73309)\n",
      "    KL (full):     0.48707 +-    0.00757 (   0.49464)\n",
      "    KL (diag):     0.10360 +-    0.00358 (   0.10719)\n",
      "Epoch 12:\n",
      "    Loglik (T):   -0.74367 +-    0.00516 (  -0.74883)\n",
      "    Loglik (V):   -0.72278 +-    0.01070 (  -0.73349)\n",
      "    KL (full):     0.48705 +-    0.00798 (   0.49503)\n",
      "    KL (diag):     0.10358 +-    0.00382 (   0.10740)\n",
      "Epoch 13:\n",
      "    Loglik (T):   -0.74102 +-    0.00523 (  -0.74625)\n",
      "    Loglik (V):   -0.72167 +-    0.01012 (  -0.73179)\n",
      "    KL (full):     0.48594 +-    0.00739 (   0.49333)\n",
      "    KL (diag):     0.10248 +-    0.00346 (   0.10594)\n",
      "Epoch 14:\n",
      "    Loglik (T):   -0.73177 +-    0.00520 (  -0.73697)\n",
      "    Loglik (V):   -0.71760 +-    0.01048 (  -0.72808)\n",
      "    KL (full):     0.48186 +-    0.00778 (   0.48965)\n",
      "    KL (diag):     0.09840 +-    0.00373 (   0.10212)\n",
      "Epoch 15:\n",
      "    Loglik (T):   -0.72711 +-    0.00509 (  -0.73219)\n",
      "    Loglik (V):   -0.72312 +-    0.01027 (  -0.73339)\n",
      "    KL (full):     0.48739 +-    0.00756 (   0.49495)\n",
      "    KL (diag):     0.10392 +-    0.00361 (   0.10753)\n",
      "Epoch 16:\n",
      "    Loglik (T):   -0.73286 +-    0.00523 (  -0.73809)\n",
      "    Loglik (V):   -0.71559 +-    0.01037 (  -0.72596)\n",
      "    KL (full):     0.47986 +-    0.00763 (   0.48749)\n",
      "    KL (diag):     0.09640 +-    0.00347 (   0.09986)\n",
      "Epoch 17:\n",
      "    Loglik (T):   -0.72487 +-    0.00527 (  -0.73013)\n",
      "    Loglik (V):   -0.71408 +-    0.01029 (  -0.72438)\n",
      "    KL (full):     0.47835 +-    0.00753 (   0.48588)\n",
      "    KL (diag):     0.09489 +-    0.00337 (   0.09826)\n",
      "Epoch 18:\n",
      "    Loglik (T):   -0.72758 +-    0.00521 (  -0.73279)\n",
      "    Loglik (V):   -0.71641 +-    0.01038 (  -0.72679)\n",
      "    KL (full):     0.48068 +-    0.00761 (   0.48829)\n",
      "    KL (diag):     0.09721 +-    0.00347 (   0.10069)\n",
      "Epoch 19:\n",
      "    Loglik (T):   -0.72510 +-    0.00529 (  -0.73039)\n",
      "    Loglik (V):   -0.71845 +-    0.00995 (  -0.72840)\n",
      "    KL (full):     0.48272 +-    0.00719 (   0.48991)\n",
      "    KL (diag):     0.09925 +-    0.00320 (   0.10245)\n",
      "Epoch 20:\n",
      "    Loglik (T):   -0.73126 +-    0.00524 (  -0.73650)\n",
      "    Loglik (V):   -0.71243 +-    0.01027 (  -0.72270)\n",
      "    KL (full):     0.47670 +-    0.00750 (   0.48420)\n",
      "    KL (diag):     0.09323 +-    0.00331 (   0.09654)\n",
      "Epoch 21:\n",
      "    Loglik (T):   -0.73017 +-    0.00516 (  -0.73533)\n",
      "    Loglik (V):   -0.70577 +-    0.01057 (  -0.71635)\n",
      "    KL (full):     0.47004 +-    0.00779 (   0.47783)\n",
      "    KL (diag):     0.08658 +-    0.00343 (   0.09001)\n",
      "Epoch 22:\n",
      "    Loglik (T):   -0.71667 +-    0.00525 (  -0.72192)\n",
      "    Loglik (V):   -0.70878 +-    0.01044 (  -0.71922)\n",
      "    KL (full):     0.47305 +-    0.00765 (   0.48071)\n",
      "    KL (diag):     0.08959 +-    0.00333 (   0.09292)\n",
      "Epoch 23:\n",
      "    Loglik (T):   -0.73218 +-    0.00531 (  -0.73749)\n",
      "    Loglik (V):   -0.70814 +-    0.01057 (  -0.71871)\n",
      "    KL (full):     0.47241 +-    0.00776 (   0.48017)\n",
      "    KL (diag):     0.08895 +-    0.00339 (   0.09234)\n",
      "Epoch 24:\n",
      "    Loglik (T):   -0.71911 +-    0.00529 (  -0.72440)\n",
      "    Loglik (V):   -0.70687 +-    0.01055 (  -0.71743)\n",
      "    KL (full):     0.47114 +-    0.00777 (   0.47891)\n",
      "    KL (diag):     0.08768 +-    0.00336 (   0.09104)\n",
      "Epoch 25:\n",
      "    Loglik (T):   -0.73361 +-    0.00531 (  -0.73892)\n",
      "    Loglik (V):   -0.71045 +-    0.01036 (  -0.72081)\n",
      "    KL (full):     0.47472 +-    0.00761 (   0.48233)\n",
      "    KL (diag):     0.09126 +-    0.00335 (   0.09460)\n",
      "Epoch 26:\n",
      "    Loglik (T):   -0.73521 +-    0.00539 (  -0.74060)\n",
      "    Loglik (V):   -0.70537 +-    0.01048 (  -0.71585)\n",
      "    KL (full):     0.46964 +-    0.00768 (   0.47733)\n",
      "    KL (diag):     0.08618 +-    0.00327 (   0.08944)\n",
      "Epoch 27:\n",
      "    Loglik (T):   -0.73599 +-    0.00529 (  -0.74128)\n",
      "    Loglik (V):   -0.70210 +-    0.01032 (  -0.71242)\n",
      "    KL (full):     0.46637 +-    0.00752 (   0.47389)\n",
      "    KL (diag):     0.08290 +-    0.00309 (   0.08599)\n",
      "Epoch 28:\n",
      "    Loglik (T):   -0.74476 +-    0.00544 (  -0.75020)\n",
      "    Loglik (V):   -0.70270 +-    0.01051 (  -0.71321)\n",
      "    KL (full):     0.46697 +-    0.00772 (   0.47469)\n",
      "    KL (diag):     0.08350 +-    0.00324 (   0.08674)\n",
      "Epoch 29:\n",
      "    Loglik (T):   -0.74583 +-    0.00535 (  -0.75118)\n",
      "    Loglik (V):   -0.69977 +-    0.01037 (  -0.71014)\n",
      "    KL (full):     0.46404 +-    0.00756 (   0.47160)\n",
      "    KL (diag):     0.08057 +-    0.00301 (   0.08358)\n",
      "Epoch 30:\n",
      "    Loglik (T):   -0.73483 +-    0.00539 (  -0.74022)\n",
      "    Loglik (V):   -0.70991 +-    0.01049 (  -0.72040)\n",
      "    KL (full):     0.47418 +-    0.00771 (   0.48189)\n",
      "    KL (diag):     0.09071 +-    0.00341 (   0.09412)\n",
      "Epoch 31:\n",
      "    Loglik (T):   -0.72379 +-    0.00535 (  -0.72914)\n",
      "    Loglik (V):   -0.70023 +-    0.01043 (  -0.71066)\n",
      "    KL (full):     0.46450 +-    0.00761 (   0.47210)\n",
      "    KL (diag):     0.08103 +-    0.00302 (   0.08405)\n",
      "Epoch 32:\n",
      "    Loglik (T):   -0.70356 +-    0.00522 (  -0.70878)\n",
      "    Loglik (V):   -0.70208 +-    0.01055 (  -0.71263)\n",
      "    KL (full):     0.46635 +-    0.00776 (   0.47411)\n",
      "    KL (diag):     0.08288 +-    0.00328 (   0.08617)\n",
      "Epoch 33:\n",
      "    Loglik (T):   -0.70856 +-    0.00527 (  -0.71382)\n",
      "    Loglik (V):   -0.70914 +-    0.01039 (  -0.71953)\n",
      "    KL (full):     0.47341 +-    0.00761 (   0.48101)\n",
      "    KL (diag):     0.08994 +-    0.00320 (   0.09314)\n",
      "Epoch 34:\n",
      "    Loglik (T):   -0.70638 +-    0.00529 (  -0.71167)\n",
      "    Loglik (V):   -0.69854 +-    0.01039 (  -0.70893)\n",
      "    KL (full):     0.46281 +-    0.00760 (   0.47040)\n",
      "    KL (diag):     0.07934 +-    0.00300 (   0.08234)\n",
      "Epoch 35:\n",
      "    Loglik (T):   -0.72547 +-    0.00535 (  -0.73082)\n",
      "    Loglik (V):   -0.69606 +-    0.01044 (  -0.70650)\n",
      "    KL (full):     0.46033 +-    0.00762 (   0.46795)\n",
      "    KL (diag):     0.07686 +-    0.00300 (   0.07986)\n",
      "Epoch 36:\n",
      "    Loglik (T):   -0.71856 +-    0.00528 (  -0.72384)\n",
      "    Loglik (V):   -0.69870 +-    0.01064 (  -0.70933)\n",
      "    KL (full):     0.46296 +-    0.00781 (   0.47077)\n",
      "    KL (diag):     0.07950 +-    0.00313 (   0.08263)\n",
      "Epoch 37:\n",
      "    Loglik (T):   -0.72973 +-    0.00543 (  -0.73516)\n",
      "    Loglik (V):   -0.70065 +-    0.01043 (  -0.71108)\n",
      "    KL (full):     0.46492 +-    0.00762 (   0.47255)\n",
      "    KL (diag):     0.08146 +-    0.00314 (   0.08460)\n",
      "Epoch 38:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loglik (T):   -0.72756 +-    0.00536 (  -0.73292)\n",
      "    Loglik (V):   -0.70797 +-    0.01045 (  -0.71841)\n",
      "    KL (full):     0.47223 +-    0.00769 (   0.47992)\n",
      "    KL (diag):     0.08877 +-    0.00333 (   0.09210)\n",
      "Epoch 39:\n",
      "    Loglik (T):   -0.71920 +-    0.00531 (  -0.72451)\n",
      "    Loglik (V):   -0.69649 +-    0.01040 (  -0.70689)\n",
      "    KL (full):     0.46075 +-    0.00757 (   0.46833)\n",
      "    KL (diag):     0.07729 +-    0.00300 (   0.08029)\n",
      "Epoch 40:\n",
      "    Loglik (T):   -0.72155 +-    0.00535 (  -0.72689)\n",
      "    Loglik (V):   -0.69557 +-    0.01050 (  -0.70607)\n",
      "    KL (full):     0.45984 +-    0.00769 (   0.46752)\n",
      "    KL (diag):     0.07637 +-    0.00304 (   0.07941)\n",
      "Epoch 41:\n",
      "    Loglik (T):   -0.71047 +-    0.00521 (  -0.71569)\n",
      "    Loglik (V):   -0.69522 +-    0.01047 (  -0.70570)\n",
      "    KL (full):     0.45949 +-    0.00765 (   0.46714)\n",
      "    KL (diag):     0.07603 +-    0.00292 (   0.07895)\n",
      "Epoch 42:\n",
      "    Loglik (T):   -0.69956 +-    0.00527 (  -0.70483)\n",
      "    Loglik (V):   -0.69263 +-    0.01059 (  -0.70322)\n",
      "    KL (full):     0.45690 +-    0.00773 (   0.46463)\n",
      "    KL (diag):     0.07343 +-    0.00298 (   0.07642)\n",
      "Epoch 43:\n",
      "    Loglik (T):   -0.70723 +-    0.00533 (  -0.71256)\n",
      "    Loglik (V):   -0.69468 +-    0.01040 (  -0.70509)\n",
      "    KL (full):     0.45895 +-    0.00756 (   0.46651)\n",
      "    KL (diag):     0.07549 +-    0.00290 (   0.07839)\n",
      "Epoch 44:\n",
      "    Loglik (T):   -0.71104 +-    0.00543 (  -0.71647)\n",
      "    Loglik (V):   -0.70324 +-    0.01045 (  -0.71369)\n",
      "    KL (full):     0.46751 +-    0.00764 (   0.47515)\n",
      "    KL (diag):     0.08404 +-    0.00323 (   0.08727)\n",
      "Epoch 45:\n",
      "    Loglik (T):   -0.70700 +-    0.00529 (  -0.71228)\n",
      "    Loglik (V):   -0.69406 +-    0.01069 (  -0.70475)\n",
      "    KL (full):     0.45833 +-    0.00787 (   0.46620)\n",
      "    KL (diag):     0.07486 +-    0.00320 (   0.07806)\n",
      "Epoch 46:\n",
      "    Loglik (T):   -0.70256 +-    0.00528 (  -0.70784)\n",
      "    Loglik (V):   -0.69108 +-    0.01068 (  -0.70176)\n",
      "    KL (full):     0.45535 +-    0.00785 (   0.46320)\n",
      "    KL (diag):     0.07189 +-    0.00301 (   0.07489)\n",
      "Epoch 47:\n",
      "    Loglik (T):   -0.69916 +-    0.00547 (  -0.70463)\n",
      "    Loglik (V):   -0.69026 +-    0.01059 (  -0.70084)\n",
      "    KL (full):     0.45452 +-    0.00773 (   0.46225)\n",
      "    KL (diag):     0.07106 +-    0.00296 (   0.07401)\n",
      "Epoch 48:\n",
      "    Loglik (T):   -0.69438 +-    0.00534 (  -0.69972)\n",
      "    Loglik (V):   -0.68856 +-    0.01051 (  -0.69908)\n",
      "    KL (full):     0.45283 +-    0.00766 (   0.46049)\n",
      "    KL (diag):     0.06937 +-    0.00285 (   0.07222)\n",
      "Epoch 49:\n",
      "    Loglik (T):   -0.71151 +-    0.00535 (  -0.71686)\n",
      "    Loglik (V):   -0.68785 +-    0.01067 (  -0.69852)\n",
      "    KL (full):     0.45212 +-    0.00780 (   0.45992)\n",
      "    KL (diag):     0.06866 +-    0.00286 (   0.07151)\n",
      "Epoch 50:\n",
      "    Loglik (T):   -0.71658 +-    0.00546 (  -0.72203)\n",
      "    Loglik (V):   -0.69013 +-    0.01042 (  -0.70055)\n",
      "    KL (full):     0.45440 +-    0.00757 (   0.46197)\n",
      "    KL (diag):     0.07093 +-    0.00278 (   0.07371)\n",
      "Epoch 51:\n",
      "    Loglik (T):   -0.71327 +-    0.00524 (  -0.71852)\n",
      "    Loglik (V):   -0.69041 +-    0.01036 (  -0.70078)\n",
      "    KL (full):     0.45468 +-    0.00753 (   0.46222)\n",
      "    KL (diag):     0.07122 +-    0.00276 (   0.07398)\n",
      "Epoch 52:\n",
      "    Loglik (T):   -0.69991 +-    0.00536 (  -0.70527)\n",
      "    Loglik (V):   -0.68921 +-    0.01068 (  -0.69989)\n",
      "    KL (full):     0.45348 +-    0.00782 (   0.46130)\n",
      "    KL (diag):     0.07001 +-    0.00297 (   0.07298)\n",
      "Epoch 53:\n",
      "    Loglik (T):   -0.71470 +-    0.00542 (  -0.72012)\n",
      "    Loglik (V):   -0.68849 +-    0.01072 (  -0.69922)\n",
      "    KL (full):     0.45276 +-    0.00787 (   0.46063)\n",
      "    KL (diag):     0.06930 +-    0.00293 (   0.07223)\n",
      "Epoch 54:\n",
      "    Loglik (T):   -0.72062 +-    0.00538 (  -0.72600)\n",
      "    Loglik (V):   -0.69837 +-    0.01041 (  -0.70878)\n",
      "    KL (full):     0.46264 +-    0.00759 (   0.47023)\n",
      "    KL (diag):     0.07918 +-    0.00304 (   0.08222)\n",
      "Epoch 55:\n",
      "    Loglik (T):   -0.70903 +-    0.00536 (  -0.71440)\n",
      "    Loglik (V):   -0.69459 +-    0.01049 (  -0.70507)\n",
      "    KL (full):     0.45885 +-    0.00766 (   0.46651)\n",
      "    KL (diag):     0.07539 +-    0.00296 (   0.07834)\n",
      "Epoch 56:\n",
      "    Loglik (T):   -0.71542 +-    0.00539 (  -0.72081)\n",
      "    Loglik (V):   -0.68644 +-    0.01065 (  -0.69709)\n",
      "    KL (full):     0.45071 +-    0.00779 (   0.45850)\n",
      "    KL (diag):     0.06724 +-    0.00289 (   0.07014)\n",
      "Epoch 57:\n",
      "    Loglik (T):   -0.70668 +-    0.00539 (  -0.71207)\n",
      "    Loglik (V):   -0.68755 +-    0.01070 (  -0.69825)\n",
      "    KL (full):     0.45182 +-    0.00783 (   0.45965)\n",
      "    KL (diag):     0.06835 +-    0.00286 (   0.07121)\n",
      "Epoch 58:\n",
      "    Loglik (T):   -0.70704 +-    0.00536 (  -0.71240)\n",
      "    Loglik (V):   -0.69718 +-    0.01067 (  -0.70785)\n",
      "    KL (full):     0.46145 +-    0.00786 (   0.46931)\n",
      "    KL (diag):     0.07799 +-    0.00321 (   0.08119)\n",
      "Epoch 59:\n",
      "    Loglik (T):   -0.71079 +-    0.00545 (  -0.71625)\n",
      "    Loglik (V):   -0.69061 +-    0.01082 (  -0.70143)\n",
      "    KL (full):     0.45487 +-    0.00798 (   0.46285)\n",
      "    KL (diag):     0.07141 +-    0.00306 (   0.07447)\n",
      "Epoch 60:\n",
      "    Loglik (T):   -0.72011 +-    0.00534 (  -0.72545)\n",
      "    Loglik (V):   -0.68975 +-    0.01041 (  -0.70017)\n",
      "    KL (full):     0.45402 +-    0.00758 (   0.46160)\n",
      "    KL (diag):     0.07056 +-    0.00275 (   0.07330)\n",
      "Epoch 61:\n",
      "    Loglik (T):   -0.69682 +-    0.00529 (  -0.70211)\n",
      "    Loglik (V):   -0.68498 +-    0.01065 (  -0.69563)\n",
      "    KL (full):     0.44925 +-    0.00778 (   0.45703)\n",
      "    KL (diag):     0.06578 +-    0.00272 (   0.06850)\n",
      "Epoch 62:\n",
      "    Loglik (T):   -0.70312 +-    0.00543 (  -0.70854)\n",
      "    Loglik (V):   -0.68623 +-    0.01061 (  -0.69684)\n",
      "    KL (full):     0.45050 +-    0.00775 (   0.45824)\n",
      "    KL (diag):     0.06703 +-    0.00277 (   0.06980)\n",
      "Epoch 63:\n",
      "    Loglik (T):   -0.70327 +-    0.00541 (  -0.70868)\n",
      "    Loglik (V):   -0.68652 +-    0.01058 (  -0.69710)\n",
      "    KL (full):     0.45079 +-    0.00775 (   0.45853)\n",
      "    KL (diag):     0.06732 +-    0.00279 (   0.07012)\n",
      "Epoch 64:\n",
      "    Loglik (T):   -0.70395 +-    0.00539 (  -0.70934)\n",
      "    Loglik (V):   -0.68461 +-    0.01061 (  -0.69522)\n",
      "    KL (full):     0.44888 +-    0.00775 (   0.45663)\n",
      "    KL (diag):     0.06542 +-    0.00278 (   0.06820)\n",
      "Epoch 65:\n",
      "    Loglik (T):   -0.70218 +-    0.00546 (  -0.70764)\n",
      "    Loglik (V):   -0.68496 +-    0.01066 (  -0.69561)\n",
      "    KL (full):     0.44923 +-    0.00778 (   0.45701)\n",
      "    KL (diag):     0.06576 +-    0.00275 (   0.06851)\n",
      "Epoch 66:\n",
      "    Loglik (T):   -0.70027 +-    0.00538 (  -0.70565)\n",
      "    Loglik (V):   -0.68888 +-    0.01042 (  -0.69930)\n",
      "    KL (full):     0.45315 +-    0.00759 (   0.46073)\n",
      "    KL (diag):     0.06968 +-    0.00274 (   0.07242)\n",
      "Epoch 67:\n",
      "    Loglik (T):   -0.70919 +-    0.00535 (  -0.71454)\n",
      "    Loglik (V):   -0.68417 +-    0.01058 (  -0.69475)\n",
      "    KL (full):     0.44844 +-    0.00772 (   0.45615)\n",
      "    KL (diag):     0.06497 +-    0.00279 (   0.06776)\n",
      "Epoch 68:\n",
      "    Loglik (T):   -0.70370 +-    0.00545 (  -0.70915)\n",
      "    Loglik (V):   -0.68219 +-    0.01059 (  -0.69278)\n",
      "    KL (full):     0.44646 +-    0.00772 (   0.45418)\n",
      "    KL (diag):     0.06300 +-    0.00269 (   0.06568)\n",
      "Epoch 69:\n",
      "    Loglik (T):   -0.70124 +-    0.00546 (  -0.70670)\n",
      "    Loglik (V):   -0.68779 +-    0.01082 (  -0.69862)\n",
      "    KL (full):     0.45206 +-    0.00796 (   0.46002)\n",
      "    KL (diag):     0.06860 +-    0.00298 (   0.07157)\n",
      "Epoch 70:\n",
      "    Loglik (T):   -0.70031 +-    0.00532 (  -0.70563)\n",
      "    Loglik (V):   -0.68271 +-    0.01074 (  -0.69345)\n",
      "    KL (full):     0.44698 +-    0.00787 (   0.45484)\n",
      "    KL (diag):     0.06351 +-    0.00273 (   0.06624)\n",
      "Epoch 71:\n",
      "    Loglik (T):   -0.70660 +-    0.00530 (  -0.71190)\n",
      "    Loglik (V):   -0.68342 +-    0.01066 (  -0.69408)\n",
      "    KL (full):     0.44768 +-    0.00777 (   0.45546)\n",
      "    KL (diag):     0.06422 +-    0.00270 (   0.06691)\n",
      "Epoch 72:\n",
      "    Loglik (T):   -0.69808 +-    0.00535 (  -0.70343)\n",
      "    Loglik (V):   -0.68669 +-    0.01042 (  -0.69711)\n",
      "    KL (full):     0.45096 +-    0.00757 (   0.45853)\n",
      "    KL (diag):     0.06750 +-    0.00268 (   0.07017)\n",
      "Epoch 73:\n",
      "    Loglik (T):   -0.70943 +-    0.00548 (  -0.71491)\n",
      "    Loglik (V):   -0.68277 +-    0.01050 (  -0.69327)\n",
      "    KL (full):     0.44704 +-    0.00763 (   0.45467)\n",
      "    KL (diag):     0.06357 +-    0.00267 (   0.06625)\n",
      "Epoch 74:\n",
      "    Loglik (T):   -0.68975 +-    0.00525 (  -0.69500)\n",
      "    Loglik (V):   -0.68256 +-    0.01065 (  -0.69321)\n",
      "    KL (full):     0.44683 +-    0.00779 (   0.45461)\n",
      "    KL (diag):     0.06336 +-    0.00269 (   0.06606)\n",
      "Epoch 75:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loglik (T):   -0.69614 +-    0.00541 (  -0.70155)\n",
      "    Loglik (V):   -0.68638 +-    0.01058 (  -0.69696)\n",
      "    KL (full):     0.45064 +-    0.00772 (   0.45836)\n",
      "    KL (diag):     0.06718 +-    0.00271 (   0.06989)\n",
      "Epoch 76:\n",
      "    Loglik (T):   -0.71482 +-    0.00541 (  -0.72023)\n",
      "    Loglik (V):   -0.68723 +-    0.01059 (  -0.69782)\n",
      "    KL (full):     0.45150 +-    0.00772 (   0.45922)\n",
      "    KL (diag):     0.06803 +-    0.00282 (   0.07085)\n",
      "Epoch 77:\n",
      "    Loglik (T):   -0.68920 +-    0.00530 (  -0.69449)\n",
      "    Loglik (V):   -0.68225 +-    0.01050 (  -0.69276)\n",
      "    KL (full):     0.44652 +-    0.00763 (   0.45415)\n",
      "    KL (diag):     0.06306 +-    0.00264 (   0.06570)\n",
      "Epoch 78:\n",
      "    Loglik (T):   -0.69099 +-    0.00536 (  -0.69635)\n",
      "    Loglik (V):   -0.68031 +-    0.01053 (  -0.69084)\n",
      "    KL (full):     0.44458 +-    0.00765 (   0.45223)\n",
      "    KL (diag):     0.06111 +-    0.00256 (   0.06367)\n",
      "Epoch 79:\n",
      "    Loglik (T):   -0.70446 +-    0.00533 (  -0.70980)\n",
      "    Loglik (V):   -0.68155 +-    0.01058 (  -0.69213)\n",
      "    KL (full):     0.44582 +-    0.00769 (   0.45351)\n",
      "    KL (diag):     0.06235 +-    0.00262 (   0.06497)\n",
      "Epoch 80:\n",
      "    Loglik (T):   -0.68811 +-    0.00530 (  -0.69340)\n",
      "    Loglik (V):   -0.68584 +-    0.01053 (  -0.69637)\n",
      "    KL (full):     0.45011 +-    0.00766 (   0.45778)\n",
      "    KL (diag):     0.06664 +-    0.00272 (   0.06937)\n",
      "Epoch 81:\n",
      "    Loglik (T):   -0.70968 +-    0.00549 (  -0.71517)\n",
      "    Loglik (V):   -0.68167 +-    0.01075 (  -0.69242)\n",
      "    KL (full):     0.44594 +-    0.00785 (   0.45379)\n",
      "    KL (diag):     0.06247 +-    0.00276 (   0.06523)\n",
      "Epoch 82:\n",
      "    Loglik (T):   -0.70631 +-    0.00548 (  -0.71179)\n",
      "    Loglik (V):   -0.68052 +-    0.01062 (  -0.69114)\n",
      "    KL (full):     0.44479 +-    0.00772 (   0.45251)\n",
      "    KL (diag):     0.06132 +-    0.00260 (   0.06392)\n",
      "Epoch 83:\n",
      "    Loglik (T):   -0.70849 +-    0.00540 (  -0.71389)\n",
      "    Loglik (V):   -0.67869 +-    0.01070 (  -0.68940)\n",
      "    KL (full):     0.44296 +-    0.00781 (   0.45077)\n",
      "    KL (diag):     0.05950 +-    0.00267 (   0.06217)\n",
      "Epoch 84:\n",
      "    Loglik (T):   -0.69602 +-    0.00544 (  -0.70146)\n",
      "    Loglik (V):   -0.68125 +-    0.01080 (  -0.69205)\n",
      "    KL (full):     0.44552 +-    0.00791 (   0.45343)\n",
      "    KL (diag):     0.06205 +-    0.00282 (   0.06487)\n",
      "Epoch 85:\n",
      "    Loglik (T):   -0.69552 +-    0.00540 (  -0.70093)\n",
      "    Loglik (V):   -0.67931 +-    0.01065 (  -0.68996)\n",
      "    KL (full):     0.44358 +-    0.00777 (   0.45135)\n",
      "    KL (diag):     0.06011 +-    0.00261 (   0.06272)\n",
      "Epoch 86:\n",
      "    Loglik (T):   -0.69441 +-    0.00546 (  -0.69987)\n",
      "    Loglik (V):   -0.68529 +-    0.01071 (  -0.69600)\n",
      "    KL (full):     0.44956 +-    0.00785 (   0.45741)\n",
      "    KL (diag):     0.06609 +-    0.00280 (   0.06889)\n",
      "Epoch 87:\n",
      "    Loglik (T):   -0.68653 +-    0.00536 (  -0.69189)\n",
      "    Loglik (V):   -0.67698 +-    0.01080 (  -0.68777)\n",
      "    KL (full):     0.44124 +-    0.00789 (   0.44914)\n",
      "    KL (diag):     0.05778 +-    0.00262 (   0.06040)\n",
      "Epoch 88:\n",
      "    Loglik (T):   -0.69311 +-    0.00539 (  -0.69850)\n",
      "    Loglik (V):   -0.67971 +-    0.01070 (  -0.69040)\n",
      "    KL (full):     0.44397 +-    0.00782 (   0.45180)\n",
      "    KL (diag):     0.06051 +-    0.00272 (   0.06323)\n",
      "Epoch 89:\n",
      "    Loglik (T):   -0.70284 +-    0.00537 (  -0.70821)\n",
      "    Loglik (V):   -0.68384 +-    0.01077 (  -0.69461)\n",
      "    KL (full):     0.44811 +-    0.00788 (   0.45599)\n",
      "    KL (diag):     0.06464 +-    0.00282 (   0.06746)\n",
      "Epoch 90:\n",
      "    Loglik (T):   -0.68657 +-    0.00529 (  -0.69186)\n",
      "    Loglik (V):   -0.67891 +-    0.01050 (  -0.68941)\n",
      "    KL (full):     0.44318 +-    0.00763 (   0.45081)\n",
      "    KL (diag):     0.05972 +-    0.00246 (   0.06217)\n",
      "Epoch 91:\n",
      "    Loglik (T):   -0.67764 +-    0.00533 (  -0.68298)\n",
      "    Loglik (V):   -0.67619 +-    0.01069 (  -0.68688)\n",
      "    KL (full):     0.44046 +-    0.00779 (   0.44825)\n",
      "    KL (diag):     0.05700 +-    0.00255 (   0.05955)\n",
      "Epoch 92:\n",
      "    Loglik (T):   -0.70279 +-    0.00540 (  -0.70819)\n",
      "    Loglik (V):   -0.68105 +-    0.01051 (  -0.69156)\n",
      "    KL (full):     0.44532 +-    0.00764 (   0.45296)\n",
      "    KL (diag):     0.06185 +-    0.00262 (   0.06448)\n",
      "Epoch 93:\n",
      "    Loglik (T):   -0.70537 +-    0.00545 (  -0.71082)\n",
      "    Loglik (V):   -0.68123 +-    0.01083 (  -0.69206)\n",
      "    KL (full):     0.44550 +-    0.00793 (   0.45343)\n",
      "    KL (diag):     0.06204 +-    0.00280 (   0.06484)\n",
      "Epoch 94:\n",
      "    Loglik (T):   -0.70310 +-    0.00550 (  -0.70860)\n",
      "    Loglik (V):   -0.67922 +-    0.01085 (  -0.69007)\n",
      "    KL (full):     0.44349 +-    0.00798 (   0.45147)\n",
      "    KL (diag):     0.06002 +-    0.00277 (   0.06280)\n",
      "Epoch 95:\n",
      "    Loglik (T):   -0.69153 +-    0.00546 (  -0.69699)\n",
      "    Loglik (V):   -0.67818 +-    0.01075 (  -0.68893)\n",
      "    KL (full):     0.44245 +-    0.00786 (   0.45031)\n",
      "    KL (diag):     0.05899 +-    0.00274 (   0.06172)\n",
      "Epoch 96:\n",
      "    Loglik (T):   -0.68965 +-    0.00537 (  -0.69502)\n",
      "    Loglik (V):   -0.68106 +-    0.01061 (  -0.69167)\n",
      "    KL (full):     0.44533 +-    0.00771 (   0.45303)\n",
      "    KL (diag):     0.06186 +-    0.00265 (   0.06451)\n",
      "Epoch 97:\n",
      "    Loglik (T):   -0.67928 +-    0.00540 (  -0.68468)\n",
      "    Loglik (V):   -0.68055 +-    0.01078 (  -0.69133)\n",
      "    KL (full):     0.44482 +-    0.00789 (   0.45271)\n",
      "    KL (diag):     0.06135 +-    0.00268 (   0.06403)\n",
      "Epoch 98:\n",
      "    Loglik (T):   -0.68260 +-    0.00529 (  -0.68788)\n",
      "    Loglik (V):   -0.68068 +-    0.01066 (  -0.69134)\n",
      "    KL (full):     0.44495 +-    0.00778 (   0.45273)\n",
      "    KL (diag):     0.06148 +-    0.00268 (   0.06416)\n",
      "Epoch 99:\n",
      "    Loglik (T):   -0.68880 +-    0.00542 (  -0.69422)\n",
      "    Loglik (V):   -0.68093 +-    0.01052 (  -0.69145)\n",
      "    KL (full):     0.44520 +-    0.00765 (   0.45285)\n",
      "    KL (diag):     0.06174 +-    0.00261 (   0.06435)\n",
      "Epoch 100:\n",
      "    Loglik (T):   -0.68310 +-    0.00525 (  -0.68835)\n",
      "    Loglik (V):   -0.67757 +-    0.01061 (  -0.68818)\n",
      "    KL (full):     0.44184 +-    0.00772 (   0.44956)\n",
      "    KL (diag):     0.05837 +-    0.00252 (   0.06089)\n"
     ]
    }
   ],
   "source": [
    "best_eval_lik = -np.inf\n",
    "\n",
    "# Setup training loop.\n",
    "opt = torch.optim.Adam(model.parameters(), args.rate)\n",
    "\n",
    "# Set regularisation high for the first epochs.\n",
    "original_epsilon = B.epsilon\n",
    "B.epsilon = config[\"epsilon_start\"]\n",
    "\n",
    "for i in range(0, args.epochs):\n",
    "    \n",
    "    with out.Section(f\"Epoch {i + 1}\"):\n",
    "        # Set regularisation to normal after the first epoch.\n",
    "        if i > 0:\n",
    "            B.epsilon = original_epsilon\n",
    "\n",
    "        # Perform an epoch.\n",
    "        if config[\"fix_noise\"] and i < config[\"fix_noise_epochs\"]:\n",
    "            fix_noise = 1e-4\n",
    "        else:\n",
    "            fix_noise = None\n",
    "        state, _ = train(\n",
    "            state,\n",
    "            model,\n",
    "            opt,\n",
    "            objective,\n",
    "            gen_train,\n",
    "            fix_noise=fix_noise,\n",
    "        )\n",
    "\n",
    "        # The epoch is done. Now evaluate.\n",
    "        state, val = eval(state, model, objective_cv, gen_cv())\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NP package training loop + own class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNPDeterministicEncoder(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(CNPDeterministicEncoder, self).__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.linears.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "\n",
    "    def forward(self, context_x, context_y):\n",
    "        \"\"\"\n",
    "        Encode training set as one vector representation\n",
    "\n",
    "        Args:\n",
    "            context_x:  batch_size x set_size x feature_dim\n",
    "            context_y:  batch_size x set_size x 1\n",
    "\n",
    "        Returns:\n",
    "            representation:\n",
    "        \"\"\"\n",
    "        encoder_input = torch.cat((context_x, context_y), dim=-1)\n",
    "\n",
    "        batch_size, set_size, filter_size = encoder_input.shape\n",
    "        x = encoder_input.view(batch_size * set_size, -1)\n",
    "        for i, linear in enumerate(self.linears[:-1]):\n",
    "            x = torch.relu(linear(x))\n",
    "        x = self.linears[-1](x)\n",
    "        x = x.view(batch_size, set_size, -1)\n",
    "        representation = x.mean(dim=1)\n",
    "        # Add number of context points to the representation? (does it help?)\n",
    "        if False:\n",
    "            representation = torch.cat((representation, set_size*torch.ones(batch_size,1,device=device)),dim=-1)\n",
    "        return representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNPDeterministicDecoder(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(CNPDeterministicDecoder, self).__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.linears.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "\n",
    "    def forward(self, representation, target_x):\n",
    "        \"\"\"\n",
    "        Take representation representation of current training set, and a target input x,\n",
    "        return the predictive distribution at x (Gaussian with mean mu and scale sigma)\n",
    "\n",
    "        Args:\n",
    "            representation: batch_size x representation_size\n",
    "            target_x: batch_size x set_size x d\n",
    "        \"\"\"\n",
    "        batch_size, set_size, d = target_x.shape\n",
    "        representation = representation.unsqueeze(1).repeat([1, set_size, 1])\n",
    "        input = torch.cat((representation, target_x), dim=-1)\n",
    "        x = input.view(batch_size * set_size, -1)\n",
    "        for i, linear in enumerate(self.linears[:-1]):\n",
    "            x = torch.relu(linear(x))\n",
    "        x = self.linears[-1](x)\n",
    "        out = x.view(batch_size, set_size, -1)\n",
    "        mu, log_sigma = torch.split(out, 1, dim=-1)\n",
    "        sigma = 0.01 + 0.99 * torch.nn.functional.softplus(log_sigma)\n",
    "        dist = torch.distributions.normal.Normal(loc=mu, scale=sigma)\n",
    "        return dist, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalEncoder(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(RelationalEncoder, self).__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.linears.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "\n",
    "    def forward(self, context_x, context_y, target_x):\n",
    "        \"\"\"\n",
    "        Encode target point as relational representation with the context set.\n",
    "\n",
    "        Args:\n",
    "            context_x:  batch_size x set_size x feature_dim\n",
    "            context_y:  batch_size x set_size x 1\n",
    "            target_x:   batch_size x target_set_size x feature_dim\n",
    "\n",
    "        Returns:\n",
    "            encoded_target_x: batch_size x target_set_size x relational_dim\n",
    "        \"\"\"\n",
    "\n",
    "        out_dim = 1\n",
    "        batch_size, set_size, feature_dim = context_x.shape\n",
    "        _, target_set_size, _ = target_x.shape\n",
    "        \n",
    "        # Compute difference between target and context set \n",
    "        # (we also concatenate y_i to the context, and 0 for the target)\n",
    "        context_xp = torch.cat((context_x, context_y), dim=-1).unsqueeze(1)\n",
    "\n",
    "        target_xp = torch.cat((target_x, torch.zeros(batch_size,target_set_size,1,device=device)), dim=-1).unsqueeze(2)\n",
    "        diff_x = (target_xp - context_xp).reshape(batch_size,-1,feature_dim + out_dim)\n",
    "\n",
    "        batch_size, diff_size, filter_size = diff_x.shape\n",
    "        x = diff_x.view(batch_size * diff_size, -1)\n",
    "\n",
    "        for i, linear in enumerate(self.linears[:-1]):\n",
    "            x = torch.relu(linear(x))\n",
    "        x = self.linears[-1](x)\n",
    "        x = x.view(batch_size, diff_size, -1)\n",
    "\n",
    "        encoded_feature_dim = x.shape[-1]\n",
    "        \n",
    "        x = torch.reshape(x,(batch_size, target_set_size, set_size, encoded_feature_dim))\n",
    "        encoded_target_x = x.mean(dim=2)\n",
    "        \n",
    "        return encoded_target_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCNP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNPDeterministicModel(nn.Module):\n",
    "    def __init__(self, relational_sizes, encoder_sizes, decoder_sizes):\n",
    "        super(RCNPDeterministicModel, self).__init__()\n",
    "        self._relational_encoder = RelationalEncoder(relational_sizes)\n",
    "        self._encoder = CNPDeterministicEncoder(encoder_sizes)\n",
    "        self._decoder = CNPDeterministicDecoder(decoder_sizes)\n",
    "\n",
    "    def forward(self, contexts, target_x, target_y=None):\n",
    "        (context_x, context_y) = contexts[0]\n",
    "        context_x = B.transpose(context_x)\n",
    "        context_y = B.transpose(context_y)\n",
    "        target_x = B.transpose(target_x)\n",
    "        target_y = B.transpose(target_y)\n",
    "        encoded_context_x = self._relational_encoder(context_x,context_y,context_x)\n",
    "        \n",
    "        representation = self._encoder(encoded_context_x, context_y)        \n",
    "        encoded_target_x = self._relational_encoder(context_x,context_y,target_x)        \n",
    "        dist, mu, sigma = self._decoder(representation, encoded_target_x)\n",
    "\n",
    "        log_p = None if target_y is None else dist.log_prob(target_y)\n",
    "        return log_p, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnp(state, model, opt, objective, gen, *, fix_noise):\n",
    "    vals = []\n",
    "    for batch in gen.epoch():\n",
    "        log_prob, _, _ = model(batch['contexts'], batch['xt'], batch['yt'])\n",
    "        log_prob = torch.sum(log_prob, dim=1)\n",
    "        log_prob = B.logsumexp(log_prob.reshape(1, -1), axis=0) - B.log(1)\n",
    "        obj = log_prob / B.cast(torch.float64, num_data(batch['xt'], batch['yt']))\n",
    "        \n",
    "        vals.append(B.to_numpy(obj))\n",
    "        val = -B.mean(obj)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        val.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    vals = B.concat(*vals)\n",
    "    out.kv(\"Loglik (T)\", exp.with_err(vals, and_lower=True))\n",
    "    return state, B.mean(vals) - 1.96 * B.std(vals) / B.sqrt(len(vals))\n",
    "\n",
    "\n",
    "def eval_rnp(state, model, objective, gen):\n",
    "    \"\"\"Perform evaluation.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        vals, kls, kls_diag = [], [], []\n",
    "        for batch in gen.epoch():\n",
    "            log_prob, pred_y, sigma = model(batch['contexts'], batch['xt'], batch['yt'])\n",
    "            log_prob = torch.sum(log_prob, dim=1)\n",
    "            log_prob = B.logsumexp(log_prob.reshape(1, -1), axis=0) - B.log(1)\n",
    "            obj = log_prob / B.cast(torch.float64, num_data(batch['xt'], batch['yt']))\n",
    "\n",
    "            # Save numbers.\n",
    "            n = nps.num_data(batch[\"xt\"], batch[\"yt\"])\n",
    "            vals.append(B.to_numpy(obj))\n",
    "            if \"pred_logpdf\" in batch:\n",
    "                kls.append(B.to_numpy(batch[\"pred_logpdf\"] / n - obj))\n",
    "            if \"pred_logpdf_diag\" in batch:\n",
    "                kls_diag.append(B.to_numpy(batch[\"pred_logpdf_diag\"] / n - obj))\n",
    "\n",
    "        # Report numbers.\n",
    "        vals = B.concat(*vals)\n",
    "        out.kv(\"Loglik (V)\", exp.with_err(vals, and_lower=True))\n",
    "        if kls:\n",
    "            out.kv(\"KL (full)\", exp.with_err(B.concat(*kls), and_upper=True))\n",
    "        if kls_diag:\n",
    "            out.kv(\"KL (diag)\", exp.with_err(B.concat(*kls_diag), and_upper=True))\n",
    "        \n",
    "\n",
    "        return state, B.mean(vals) - 1.96 * B.std(vals) / B.sqrt(len(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Sizes of the layers of the MLPs for the encoder and decoder\n",
    "# The final output layer of the decoder outputs two values, one for the mean and\n",
    "# one for the variance of the prediction at the target location\n",
    "d_x, d_in, representation_size, relational_size, d_out = 1, 2, 128, 64, 2\n",
    "relational_sizes = [d_in, 128, 128, relational_size]\n",
    "encoder_sizes = [relational_size + 1, 128, 128, 128, representation_size]\n",
    "decoder_sizes = [representation_size + relational_size, 128, 128, 2]\n",
    "\n",
    "original_model = RCNPDeterministicModel(relational_sizes, encoder_sizes, decoder_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "    Loglik (T):   -1.19819 +-    0.00480 (  -1.20300)\n",
      "    Loglik (V):   -1.03212 +-    0.00920 (  -1.04133)\n",
      "    KL (full):     0.71577 +-    0.00786 (   0.72363)\n",
      "    KL (diag):     0.34565 +-    0.00736 (   0.35301)\n",
      "Epoch 2:\n",
      "    Loglik (T):   -0.95546 +-    0.00486 (  -0.96032)\n",
      "    Loglik (V):   -0.86583 +-    0.00967 (  -0.87550)\n",
      "    KL (full):     0.62746 +-    0.00767 (   0.63513)\n",
      "    KL (diag):     0.23437 +-    0.00639 (   0.24075)\n",
      "Epoch 3:\n",
      "    Loglik (T):   -0.83715 +-    0.00497 (  -0.84212)\n",
      "    Loglik (V):   -0.82150 +-    0.01017 (  -0.83166)\n",
      "    KL (full):     0.58313 +-    0.00790 (   0.59103)\n",
      "    KL (diag):     0.19004 +-    0.00527 (   0.19531)\n",
      "Epoch 4:\n",
      "    Loglik (T):   -0.80517 +-    0.00496 (  -0.81013)\n",
      "    Loglik (V):   -0.78249 +-    0.00997 (  -0.79246)\n",
      "    KL (full):     0.54412 +-    0.00748 (   0.55161)\n",
      "    KL (diag):     0.15103 +-    0.00476 (   0.15579)\n",
      "Epoch 5:\n",
      "    Loglik (T):   -0.78505 +-    0.00509 (  -0.79014)\n",
      "    Loglik (V):   -0.77427 +-    0.00984 (  -0.78411)\n",
      "    KL (full):     0.53590 +-    0.00732 (   0.54322)\n",
      "    KL (diag):     0.14281 +-    0.00443 (   0.14723)\n"
     ]
    }
   ],
   "source": [
    "best_eval_lik = -np.inf\n",
    "\n",
    "# Setup training loop.\n",
    "opt = torch.optim.Adam(original_model.parameters(), args.rate)\n",
    "\n",
    "# Set regularisation high for the first epochs.\n",
    "original_epsilon = B.epsilon\n",
    "B.epsilon = config[\"epsilon_start\"]\n",
    "\n",
    "for i in range(0, args.epochs):\n",
    "    with out.Section(f\"Epoch {i + 1}\"):\n",
    "        # Set regularisation to normal after the first epoch.\n",
    "        if i > 0:\n",
    "            B.epsilon = original_epsilon\n",
    "\n",
    "        # Perform an epoch.\n",
    "        if config[\"fix_noise\"] and i < config[\"fix_noise_epochs\"]:\n",
    "            fix_noise = 1e-4\n",
    "        else:\n",
    "            fix_noise = None\n",
    "        state, _ = train_rnp(\n",
    "            state,\n",
    "            original_model,\n",
    "            opt,\n",
    "            objective,\n",
    "            gen_train,\n",
    "            fix_noise=fix_noise,\n",
    "        )\n",
    "\n",
    "        # The epoch is done. Now evaluate.\n",
    "        state, val = eval_rnp(state, original_model, objective_cv, gen_cv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-np]",
   "language": "python",
   "name": "conda-env-.conda-np-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
