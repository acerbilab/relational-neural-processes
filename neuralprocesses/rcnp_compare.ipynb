{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to compare the performance between original RCNP and our implementation is to replace the train function and its corresponding loglik objective with our own methods, while remaining other functions the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D regression task\n",
    "\n",
    "We start by importing the necessary dependencies. This implementation is based on PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "import experiment as exp\n",
    "import lab as B\n",
    "import wbml.out as out\n",
    "from matrix.util import ToDenseWarning\n",
    "from wbml.experiment import WorkingDirectory\n",
    "import neuralprocesses.torch as nps\n",
    "from neuralprocesses.numdata import num_data\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\") if USE_CUDA else torch.device(\"cpu\")\n",
    "\n",
    "state = B.create_random_state(torch.float32, seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"default\": {\n",
    "            \"epochs\": None,\n",
    "            \"rate\": None,\n",
    "            \"also_ar\": False,\n",
    "        },\n",
    "        \"epsilon\": 1e-8,\n",
    "        \"epsilon_start\": 1e-2,\n",
    "        \"cholesky_retry_factor\": 1e6,\n",
    "        \"fix_noise\": None,\n",
    "        \"fix_noise_epochs\": 3,\n",
    "        \"width\": 256,\n",
    "        \"dim_embedding\": 256,\n",
    "        \"relational_width\": 64,\n",
    "        \"dim_relational_embeddings\": 128,\n",
    "        \"enc_same\": False,\n",
    "        \"num_heads\": 8,\n",
    "        \"num_layers\": 6,\n",
    "        \"num_relational_layers\": 3,\n",
    "        \"unet_channels\": (64,) * 6,\n",
    "        \"unet_strides\": (1,) + (2,) * 5,\n",
    "        \"conv_channels\": 64,\n",
    "        \"encoder_scales\": None,\n",
    "        \"fullconvgnp_kernel_factor\": 2,\n",
    "        \"mean_diff\": 0,\n",
    "        # Performance of the ConvGNP is sensitive to this parameter. Moreover, it\n",
    "        # doesn't make sense to set it to a value higher of the last hidden layer of\n",
    "        # the CNN architecture. We therefore set it to 64.\n",
    "        \"num_basis_functions\": 64,\n",
    "        \"dim_x\": 1\n",
    "    }\n",
    "\n",
    "args = {\"dim_x\": 1,\n",
    "        \"dim_y\": 1,\n",
    "        \"data\": 'eq',\n",
    "        \"batch_size\": 16,\n",
    "        \"epochs\": 5,\n",
    "        \"rate\": 3e-4,\n",
    "        \"objective\": \"loglik\",\n",
    "        \"num_samples\": 20,\n",
    "        \"unnormalised\": False,\n",
    "        \"evaluate_num_samples\": 512,\n",
    "        \"evaluate_batch_size\": 8,\n",
    "        \"train_fast\": False,\n",
    "        \"evaluate_fast\": True,\n",
    "      \n",
    "        \n",
    "       }\n",
    "class mydict(dict):\n",
    "    def __getattribute__(self, key):\n",
    "        if key in self:\n",
    "            return self[key]\n",
    "        else:\n",
    "            return super().__getattribute__(key)    \n",
    "        \n",
    "args = mydict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define below some global variables for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train, gen_cv, gens_eval = exp.data[args.data][\"setup\"](\n",
    "        args,\n",
    "        config,\n",
    "        num_tasks_train=2**6 if args.train_fast else 2**14,\n",
    "        num_tasks_cv=2**6 if args.train_fast else 2**12,\n",
    "        num_tasks_eval=2**6 if args.evaluate_fast else 2**12,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NP package train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = partial(\n",
    "            nps.loglik,\n",
    "            num_samples=args.num_samples,\n",
    "            normalise=not args.unnormalised,\n",
    "        )\n",
    "objective_cv = partial(\n",
    "            nps.loglik,\n",
    "            num_samples=args.num_samples,\n",
    "            normalise=not args.unnormalised,\n",
    "        )\n",
    "objectives_eval = [\n",
    "            (\n",
    "                \"Loglik\",\n",
    "                partial(\n",
    "                    nps.loglik,\n",
    "                    num_samples=args.evaluate_num_samples,\n",
    "                    batch_size=args.evaluate_batch_size,\n",
    "                    normalise=not args.unnormalised,\n",
    "                ),\n",
    "            )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(state, model, opt, objective, gen, *, fix_noise):\n",
    "    \"\"\"Train for an epoch.\"\"\"\n",
    "    vals = []\n",
    "    for batch in gen.epoch():\n",
    "        state, obj = objective(\n",
    "            state,\n",
    "            model,\n",
    "            batch[\"contexts\"],\n",
    "            batch[\"xt\"],\n",
    "            batch[\"yt\"],\n",
    "            fix_noise=fix_noise,\n",
    "        )\n",
    "        vals.append(B.to_numpy(obj))\n",
    "        # Be sure to negate the output of `objective`.\n",
    "        val = -B.mean(obj)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        val.backward()\n",
    "        opt.step()\n",
    "\n",
    "    vals = B.concat(*vals)\n",
    "    out.kv(\"Loglik (T)\", exp.with_err(vals, and_lower=True))\n",
    "    return state, B.mean(vals) - 1.96 * B.std(vals) / B.sqrt(len(vals))\n",
    "\n",
    "def eval(state, model, objective, gen):\n",
    "    \"\"\"Perform evaluation.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        vals, kls, kls_diag = [], [], []\n",
    "        for batch in gen.epoch():\n",
    "            state, obj = objective(\n",
    "                state,\n",
    "                model,\n",
    "                batch[\"contexts\"],\n",
    "                batch[\"xt\"],\n",
    "                batch[\"yt\"],\n",
    "            )\n",
    "\n",
    "            # Save numbers.\n",
    "            n = nps.num_data(batch[\"xt\"], batch[\"yt\"])\n",
    "            vals.append(B.to_numpy(obj))\n",
    "            if \"pred_logpdf\" in batch:\n",
    "                kls.append(B.to_numpy(batch[\"pred_logpdf\"] / n - obj))\n",
    "            if \"pred_logpdf_diag\" in batch:\n",
    "                kls_diag.append(B.to_numpy(batch[\"pred_logpdf_diag\"] / n - obj))\n",
    "\n",
    "        # Report numbers.\n",
    "        vals = B.concat(*vals)\n",
    "        out.kv(\"Loglik (V)\", exp.with_err(vals, and_lower=True))\n",
    "        if kls:\n",
    "            out.kv(\"KL (full)\", exp.with_err(B.concat(*kls), and_upper=True))\n",
    "        if kls_diag:\n",
    "            out.kv(\"KL (diag)\", exp.with_err(B.concat(*kls_diag), and_upper=True))\n",
    "        \n",
    "        # objective doesn't return pred_y, we can't plot the data\n",
    "\n",
    "        return state, B.mean(vals) - 1.96 * B.std(vals) / B.sqrt(len(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.epsilon = config['epsilon']\n",
    "\n",
    "model = nps.construct_rnp(\n",
    "                dim_x=config[\"dim_x\"],\n",
    "                dim_yc=(1,) * config[\"dim_y\"],\n",
    "                dim_yt=config[\"dim_y\"],\n",
    "                dim_embedding=config[\"dim_embedding\"],\n",
    "                enc_same=config[\"enc_same\"],\n",
    "                num_dec_layers=config[\"num_layers\"],\n",
    "                width=config[\"width\"],\n",
    "                relational_width=config['relational_width'],\n",
    "                num_relational_enc_layers=config['num_relational_layers'],\n",
    "                likelihood=\"het\",\n",
    "                transform=config[\"transform\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daolang Huang\\.conda\\envs\\np\\Lib\\site-packages\\stheno\\mo\\input.py:9: ToDenseWarning: Could not preserve structure in block matrix: converting to dense.\n",
      "  return B.block(*[[pairwise(k, xi, yi) for yi in y] for xi in x])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loglik (T):   -1.23679 +-    0.00482 (  -1.24162)\n",
      "    Loglik (V):   -0.99596 +-    0.00935 (  -1.00532)\n",
      "    KL (full):     0.67961 +-    0.00777 (   0.68738)\n",
      "    KL (diag):     0.30949 +-    0.00680 (   0.31630)\n",
      "Epoch 2:\n",
      "    Loglik (T):   -0.92210 +-    0.00488 (  -0.92698)\n",
      "    Loglik (V):   -0.83953 +-    0.00888 (  -0.84841)\n",
      "    KL (full):     0.60116 +-    0.00663 (   0.60779)\n",
      "    KL (diag):     0.20807 +-    0.00514 (   0.21321)\n",
      "Epoch 3:\n",
      "    Loglik (T):   -0.80291 +-    0.00502 (  -0.80793)\n",
      "    Loglik (V):   -0.81952 +-    0.00974 (  -0.82925)\n",
      "    KL (full):     0.58115 +-    0.00743 (   0.58858)\n",
      "    KL (diag):     0.18806 +-    0.00544 (   0.19350)\n",
      "Epoch 4:\n",
      "    Loglik (T):   -0.78426 +-    0.00515 (  -0.78941)\n",
      "    Loglik (V):   -0.77612 +-    0.01065 (  -0.78677)\n",
      "    KL (full):     0.53775 +-    0.00818 (   0.54593)\n",
      "    KL (diag):     0.14466 +-    0.00480 (   0.14946)\n",
      "Epoch 5:\n",
      "    Loglik (T):   -0.76835 +-    0.00507 (  -0.77342)\n",
      "    Loglik (V):   -0.74898 +-    0.01038 (  -0.75936)\n",
      "    KL (full):     0.51062 +-    0.00776 (   0.51838)\n",
      "    KL (diag):     0.11753 +-    0.00403 (   0.12156)\n"
     ]
    }
   ],
   "source": [
    "best_eval_lik = -np.inf\n",
    "\n",
    "# Setup training loop.\n",
    "opt = torch.optim.Adam(model.parameters(), args.rate)\n",
    "\n",
    "# Set regularisation high for the first epochs.\n",
    "original_epsilon = B.epsilon\n",
    "B.epsilon = config[\"epsilon_start\"]\n",
    "\n",
    "for i in range(0, args.epochs):\n",
    "    with out.Section(f\"Epoch {i + 1}\"):\n",
    "        # Set regularisation to normal after the first epoch.\n",
    "        if i > 0:\n",
    "            B.epsilon = original_epsilon\n",
    "\n",
    "        # Perform an epoch.\n",
    "        if config[\"fix_noise\"] and i < config[\"fix_noise_epochs\"]:\n",
    "            fix_noise = 1e-4\n",
    "        else:\n",
    "            fix_noise = None\n",
    "        state, _ = train(\n",
    "            state,\n",
    "            model,\n",
    "            opt,\n",
    "            objective,\n",
    "            gen_train,\n",
    "            fix_noise=fix_noise,\n",
    "        )\n",
    "\n",
    "        # The epoch is done. Now evaluate.\n",
    "        state, val = eval(state, model, objective_cv, gen_cv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NP package training loop + own class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNPDeterministicEncoder(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(CNPDeterministicEncoder, self).__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.linears.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "\n",
    "    def forward(self, context_x, context_y):\n",
    "        \"\"\"\n",
    "        Encode training set as one vector representation\n",
    "\n",
    "        Args:\n",
    "            context_x:  batch_size x set_size x feature_dim\n",
    "            context_y:  batch_size x set_size x 1\n",
    "\n",
    "        Returns:\n",
    "            representation:\n",
    "        \"\"\"\n",
    "        encoder_input = torch.cat((context_x, context_y), dim=-1)\n",
    "\n",
    "        batch_size, set_size, filter_size = encoder_input.shape\n",
    "        x = encoder_input.view(batch_size * set_size, -1)\n",
    "        for i, linear in enumerate(self.linears[:-1]):\n",
    "            x = torch.relu(linear(x))\n",
    "        x = self.linears[-1](x)\n",
    "        x = x.view(batch_size, set_size, -1)\n",
    "        representation = x.mean(dim=1)\n",
    "        # Add number of context points to the representation? (does it help?)\n",
    "        if False:\n",
    "            representation = torch.cat((representation, set_size*torch.ones(batch_size,1,device=device)),dim=-1)\n",
    "        return representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNPDeterministicDecoder(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(CNPDeterministicDecoder, self).__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.linears.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "\n",
    "    def forward(self, representation, target_x):\n",
    "        \"\"\"\n",
    "        Take representation representation of current training set, and a target input x,\n",
    "        return the predictive distribution at x (Gaussian with mean mu and scale sigma)\n",
    "\n",
    "        Args:\n",
    "            representation: batch_size x representation_size\n",
    "            target_x: batch_size x set_size x d\n",
    "        \"\"\"\n",
    "        batch_size, set_size, d = target_x.shape\n",
    "        representation = representation.unsqueeze(1).repeat([1, set_size, 1])\n",
    "        input = torch.cat((representation, target_x), dim=-1)\n",
    "        x = input.view(batch_size * set_size, -1)\n",
    "        for i, linear in enumerate(self.linears[:-1]):\n",
    "            x = torch.relu(linear(x))\n",
    "        x = self.linears[-1](x)\n",
    "        out = x.view(batch_size, set_size, -1)\n",
    "        mu, log_sigma = torch.split(out, 1, dim=-1)\n",
    "        sigma = 0.01 + 0.99 * torch.nn.functional.softplus(log_sigma)\n",
    "        dist = torch.distributions.normal.Normal(loc=mu, scale=sigma)\n",
    "        return dist, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalEncoder(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(RelationalEncoder, self).__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.linears.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "\n",
    "    def forward(self, context_x, context_y, target_x):\n",
    "        \"\"\"\n",
    "        Encode target point as relational representation with the context set.\n",
    "\n",
    "        Args:\n",
    "            context_x:  batch_size x set_size x feature_dim\n",
    "            context_y:  batch_size x set_size x 1\n",
    "            target_x:   batch_size x target_set_size x feature_dim\n",
    "\n",
    "        Returns:\n",
    "            encoded_target_x: batch_size x target_set_size x relational_dim\n",
    "        \"\"\"\n",
    "\n",
    "        out_dim = 1\n",
    "        batch_size, set_size, feature_dim = context_x.shape\n",
    "        _, target_set_size, _ = target_x.shape\n",
    "        \n",
    "        # Compute difference between target and context set \n",
    "        # (we also concatenate y_i to the context, and 0 for the target)\n",
    "        context_xp = torch.cat((context_x, context_y), dim=-1).unsqueeze(1)\n",
    "\n",
    "        target_xp = torch.cat((target_x, torch.zeros(batch_size,target_set_size,1,device=device)), dim=-1).unsqueeze(2)\n",
    "        diff_x = (target_xp - context_xp).reshape(batch_size,-1,feature_dim + out_dim)\n",
    "\n",
    "        batch_size, diff_size, filter_size = diff_x.shape\n",
    "        x = diff_x.view(batch_size * diff_size, -1)\n",
    "\n",
    "        for i, linear in enumerate(self.linears[:-1]):\n",
    "            x = torch.relu(linear(x))\n",
    "        x = self.linears[-1](x)\n",
    "        x = x.view(batch_size, diff_size, -1)\n",
    "\n",
    "        encoded_feature_dim = x.shape[-1]\n",
    "        \n",
    "        x = torch.reshape(x,(batch_size, target_set_size, set_size, encoded_feature_dim))\n",
    "        encoded_target_x = x.mean(dim=2)\n",
    "        \n",
    "        return encoded_target_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCNP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNPDeterministicModel(nn.Module):\n",
    "    def __init__(self, relational_sizes, encoder_sizes, decoder_sizes):\n",
    "        super(RCNPDeterministicModel, self).__init__()\n",
    "        self._relational_encoder = RelationalEncoder(relational_sizes)\n",
    "        self._encoder = CNPDeterministicEncoder(encoder_sizes)\n",
    "        self._decoder = CNPDeterministicDecoder(decoder_sizes)\n",
    "\n",
    "    def forward(self, contexts, target_x, target_y=None):\n",
    "        (context_x, context_y) = contexts[0]\n",
    "        context_x = B.transpose(context_x)\n",
    "        context_y = B.transpose(context_y)\n",
    "        target_x = B.transpose(target_x)\n",
    "        target_y = B.transpose(target_y)\n",
    "        encoded_context_x = self._relational_encoder(context_x,context_y,context_x)\n",
    "        \n",
    "        representation = self._encoder(encoded_context_x, context_y)        \n",
    "        encoded_target_x = self._relational_encoder(context_x,context_y,target_x)        \n",
    "        dist, mu, sigma = self._decoder(representation, encoded_target_x)\n",
    "\n",
    "        log_p = None if target_y is None else dist.log_prob(target_y)\n",
    "        return log_p, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnp(state, model, opt, objective, gen, *, fix_noise):\n",
    "    vals = []\n",
    "    for batch in gen.epoch():\n",
    "        log_prob, _, _ = model(batch['contexts'], batch['xt'], batch['yt'])\n",
    "        log_prob = torch.sum(log_prob, dim=1)\n",
    "        log_prob = B.logsumexp(log_prob.reshape(1, -1), axis=0) - B.log(1)\n",
    "        obj = log_prob / B.cast(torch.float64, num_data(batch['xt'], batch['yt']))\n",
    "        \n",
    "        vals.append(B.to_numpy(obj))\n",
    "        val = -B.mean(obj)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        val.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    vals = B.concat(*vals)\n",
    "    out.kv(\"Loglik (T)\", exp.with_err(vals, and_lower=True))\n",
    "    return state, B.mean(vals) - 1.96 * B.std(vals) / B.sqrt(len(vals))\n",
    "\n",
    "\n",
    "def eval_rnp(state, model, objective, gen):\n",
    "    \"\"\"Perform evaluation.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        vals, kls, kls_diag = [], [], []\n",
    "        for batch in gen.epoch():\n",
    "            log_prob, pred_y, sigma = model(batch['contexts'], batch['xt'], batch['yt'])\n",
    "            log_prob = torch.sum(log_prob, dim=1)\n",
    "            log_prob = B.logsumexp(log_prob.reshape(1, -1), axis=0) - B.log(1)\n",
    "            obj = log_prob / B.cast(torch.float64, num_data(batch['xt'], batch['yt']))\n",
    "\n",
    "            # Save numbers.\n",
    "            n = nps.num_data(batch[\"xt\"], batch[\"yt\"])\n",
    "            vals.append(B.to_numpy(obj))\n",
    "            if \"pred_logpdf\" in batch:\n",
    "                kls.append(B.to_numpy(batch[\"pred_logpdf\"] / n - obj))\n",
    "            if \"pred_logpdf_diag\" in batch:\n",
    "                kls_diag.append(B.to_numpy(batch[\"pred_logpdf_diag\"] / n - obj))\n",
    "\n",
    "        # Report numbers.\n",
    "        vals = B.concat(*vals)\n",
    "        out.kv(\"Loglik (V)\", exp.with_err(vals, and_lower=True))\n",
    "        if kls:\n",
    "            out.kv(\"KL (full)\", exp.with_err(B.concat(*kls), and_upper=True))\n",
    "        if kls_diag:\n",
    "            out.kv(\"KL (diag)\", exp.with_err(B.concat(*kls_diag), and_upper=True))\n",
    "        \n",
    "\n",
    "        return state, B.mean(vals) - 1.96 * B.std(vals) / B.sqrt(len(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Sizes of the layers of the MLPs for the encoder and decoder\n",
    "# The final output layer of the decoder outputs two values, one for the mean and\n",
    "# one for the variance of the prediction at the target location\n",
    "d_x, d_in, representation_size, relational_size, d_out = 1, 2, 128, 64, 2\n",
    "relational_sizes = [d_in, 128, 128, relational_size]\n",
    "encoder_sizes = [relational_size + 1, 128, 128, 128, representation_size]\n",
    "decoder_sizes = [representation_size + relational_size, 128, 128, 2]\n",
    "\n",
    "original_model = RCNPDeterministicModel(relational_sizes, encoder_sizes, decoder_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "    Loglik (T):   -1.19819 +-    0.00480 (  -1.20300)\n",
      "    Loglik (V):   -1.03212 +-    0.00920 (  -1.04133)\n",
      "    KL (full):     0.71577 +-    0.00786 (   0.72363)\n",
      "    KL (diag):     0.34565 +-    0.00736 (   0.35301)\n",
      "Epoch 2:\n",
      "    Loglik (T):   -0.95546 +-    0.00486 (  -0.96032)\n",
      "    Loglik (V):   -0.86583 +-    0.00967 (  -0.87550)\n",
      "    KL (full):     0.62746 +-    0.00767 (   0.63513)\n",
      "    KL (diag):     0.23437 +-    0.00639 (   0.24075)\n",
      "Epoch 3:\n",
      "    Loglik (T):   -0.83715 +-    0.00497 (  -0.84212)\n",
      "    Loglik (V):   -0.82150 +-    0.01017 (  -0.83166)\n",
      "    KL (full):     0.58313 +-    0.00790 (   0.59103)\n",
      "    KL (diag):     0.19004 +-    0.00527 (   0.19531)\n",
      "Epoch 4:\n",
      "    Loglik (T):   -0.80517 +-    0.00496 (  -0.81013)\n",
      "    Loglik (V):   -0.78249 +-    0.00997 (  -0.79246)\n",
      "    KL (full):     0.54412 +-    0.00748 (   0.55161)\n",
      "    KL (diag):     0.15103 +-    0.00476 (   0.15579)\n",
      "Epoch 5:\n",
      "    Loglik (T):   -0.78505 +-    0.00509 (  -0.79014)\n",
      "    Loglik (V):   -0.77427 +-    0.00984 (  -0.78411)\n",
      "    KL (full):     0.53590 +-    0.00732 (   0.54322)\n",
      "    KL (diag):     0.14281 +-    0.00443 (   0.14723)\n"
     ]
    }
   ],
   "source": [
    "best_eval_lik = -np.inf\n",
    "\n",
    "# Setup training loop.\n",
    "opt = torch.optim.Adam(original_model.parameters(), args.rate)\n",
    "\n",
    "# Set regularisation high for the first epochs.\n",
    "original_epsilon = B.epsilon\n",
    "B.epsilon = config[\"epsilon_start\"]\n",
    "\n",
    "for i in range(0, args.epochs):\n",
    "    with out.Section(f\"Epoch {i + 1}\"):\n",
    "        # Set regularisation to normal after the first epoch.\n",
    "        if i > 0:\n",
    "            B.epsilon = original_epsilon\n",
    "\n",
    "        # Perform an epoch.\n",
    "        if config[\"fix_noise\"] and i < config[\"fix_noise_epochs\"]:\n",
    "            fix_noise = 1e-4\n",
    "        else:\n",
    "            fix_noise = None\n",
    "        state, _ = train_rnp(\n",
    "            state,\n",
    "            original_model,\n",
    "            opt,\n",
    "            objective,\n",
    "            gen_train,\n",
    "            fix_noise=fix_noise,\n",
    "        )\n",
    "\n",
    "        # The epoch is done. Now evaluate.\n",
    "        state, val = eval_rnp(state, original_model, objective_cv, gen_cv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-np]",
   "language": "python",
   "name": "conda-env-.conda-np-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
